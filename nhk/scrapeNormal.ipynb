{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, json, csv, requests, time, glob, tqdm, datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_one_new(html, url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    json_data = json.loads(soup.find_all(\"script\", type=\"application/ld+json\")[-1].text)\n",
    "    title = json_data.get('headline', soup.find('span', class_='contentTitle').text)\n",
    "    date = json_data.get('datePublished', re.search(r'datetime:.*?(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2})', str(html)).group(1))\n",
    "    date_m = json_data.get('dateModified', '')\n",
    "    genre = json_data.get('genre', [])\n",
    "    if genre == []:\n",
    "        genre = [k for k in soup.find('meta', attrs={'name':'keywords'}).get('content').split(',') if k not in ['NHK','ニュース', 'NHK NEWS WEB']]\n",
    "    keywords = json_data.get('keywords', [])\n",
    "    article = soup.find('div', id=\"news_textbody\").text\n",
    "    if soup.find_all('div', id=\"news_textmore\") != []:\n",
    "        for textmore in soup.find_all('div', id=\"news_textmore\"):\n",
    "            article += ('\\n' + textmore.text)\n",
    "    if soup.find_all('div', class_=\"news_add\") != []:\n",
    "        for newsadd in soup.find_all('div', class_=\"news_add\"):\n",
    "            if newsadd.h3 != None:\n",
    "                newsadd.h3.extract()\n",
    "            article += ('\\n' + newsadd.text)\n",
    "            \n",
    "    return {\n",
    "        'id':url.split('/')[-1].split('.html')[0],\n",
    "        'title':title,\n",
    "        'article':article.strip(),\n",
    "        'genre':genre,\n",
    "        'keywords':keywords,\n",
    "        'url':url,\n",
    "        'datePublished':date,\n",
    "        'dateModified':date_m\n",
    "    }\n",
    "\n",
    "# for old web normal\n",
    "def make_date_normal_old(ymd,time):\n",
    "    year, month, day = ymd[:4], ymd[4:6], ymd[6:]\n",
    "    hour, minute = time.split('時')\n",
    "    minute = minute.strip('分')\n",
    "    if len(hour) == 1:\n",
    "        hour = '0' + hour\n",
    "    if len(minute) == 1:\n",
    "        minute = '0' + minute\n",
    "    return f\"{year}-{month}-{day}T{hour}:{minute}\"\n",
    "\n",
    "def scrape_one_old(html, url_true):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    title = soup.find('span', class_=\"contentTitle\").text.strip()\n",
    "    ymd_ = url_true.split('/')[-2]\n",
    "    time_ = soup.find('span', id=\"news_time\").text\n",
    "    date = make_date_normal_old(ymd_, time_)\n",
    "    genre = [k for k in soup.find('meta', attrs={'name':'keywords'}).get('content').split(',') if k not in ['NHK','ニュース', 'NHK NEWS WEB','ＮＨＫ','ＮＨＫニュース','']]\n",
    "    article = soup.find(['div','p'], id=\"news_textbody\").text\n",
    "    if soup.find_all(['div','p'], id=\"news_textmore\") != []:\n",
    "        for textmore in soup.find_all(['div','p'], id=\"news_textmore\"):\n",
    "            article += ('\\n' + textmore.text)\n",
    "    if soup.find_all(['div','p'], class_=\"news_add\") != []:\n",
    "        for newsadd in soup.find_all(['div','p'], class_=\"news_add\"):\n",
    "            if newsadd.h3 != None:\n",
    "                newsadd.h3.extract()\n",
    "            article += ('\\n' + newsadd.text)\n",
    "            \n",
    "    return {\n",
    "        'id':url_true.split('/')[-1].split('.html')[0],\n",
    "        'title':title,\n",
    "        'article':article.strip(),\n",
    "        'genre':genre,\n",
    "        'keywords':[],\n",
    "        'url':url_true,\n",
    "        'datePublished':date,\n",
    "        'dateModified':\"\"\n",
    "    }\n",
    "\n",
    "def write_nolink(ID, year):\n",
    "    with open(f'nolinknormal{year}.txt', 'a') as f:\n",
    "        f.write(str(ID) + '\\n')\n",
    "\n",
    "def get_archiveurl_from_id(ID, date, http=True):\n",
    "    if http:\n",
    "        url = f'https://web.archive.org/web/*/http://www3.nhk.or.jp/news/html/{date}/k{ID}1000.html'\n",
    "    else:\n",
    "        url = f'https://web.archive.org/web/*/https://www3.nhk.or.jp/news/html/{date}/k{ID}1000.html'\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    html = str(driver.page_source.encode('utf-8'))\n",
    "    snap = re.search(r'(times between|1 time|times).*?<a href=\"(.+?)\">', html)\n",
    "    print(snap)\n",
    "    try:\n",
    "        archiveurl = 'https://web.archive.org' + snap.group(2)\n",
    "    except:\n",
    "        time.sleep(5)\n",
    "        archiveurl = 'https://web.archive.org' + snap.group(2)\n",
    "    return None if 'nhk' not in archiveurl else archiveurl\n",
    "\n",
    "def get_article_from_archiveurl(archiveurl, browser=True):\n",
    "    if browser == False:\n",
    "        response = requests.get(archiveurl)\n",
    "        html = response.text\n",
    "    else:\n",
    "        driver.get(archiveurl)\n",
    "        time.sleep(3)\n",
    "        html = driver.page_source.encode('utf-8')\n",
    "\n",
    "    url_true = 'http' + archiveurl.split('/http')[-1]\n",
    "    if 'This page is not available on the web' in str(html):\n",
    "        return None\n",
    "    try:\n",
    "        try:\n",
    "            dic = scrape_one_old(html, url_true)\n",
    "        except:\n",
    "            dic = scrape_one_new(html, url_true)\n",
    "        return dic\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def read_json(year):\n",
    "    data = pd.read_json(f'nhkweb{year}.json')\n",
    "    data = data[['id', 'url']]\n",
    "    data.id = data.id.apply(lambda x:x[1:-4])\n",
    "    data['date'] = data.url.apply(lambda x:x.split('news/html/')[-1].split('/')[0])\n",
    "    print('length:', len(data))\n",
    "    return data\n",
    "    \n",
    "\n",
    "def js(dic, year):\n",
    "    if dic == None:\n",
    "        return\n",
    "    with open(f'nhkweb{year}.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    with open(f'nhkweb{year}.json', 'w', encoding='utf-8') as f:\n",
    "        if dic['id'] not in [x['id'] for x in data]:\n",
    "            data.append(dic)\n",
    "        else:\n",
    "            for i, d in enumerate(data):\n",
    "                if dic['id'] == d['id']:\n",
    "                    data[i] = dic\n",
    "        data = sorted(data, key=lambda x:x['id'])\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def datebefore(ymd:str):\n",
    "    year, month, day = int(ymd[:4]), int(ymd[4:6]), int(ymd[6:])\n",
    "    before = str(datetime.datetime(year, month, day) + datetime.timedelta(-1))\n",
    "    year, month, day = before[:4], before[5:7], before[8:10]\n",
    "    return f'{year}{month}{day}'\n",
    "\n",
    "def dateafter(ymd:str):\n",
    "    year, month, day = int(ymd[:4]), int(ymd[4:6]), int(ymd[6:])\n",
    "    before = str(datetime.datetime(year, month, day) + datetime.timedelta(1))\n",
    "    year, month, day = before[:4], before[5:7], before[8:10]\n",
    "    return f'{year}{month}{day}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open selenium & scrape manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'k10011630731000',\n",
       " 'title': '北海道の電力 冬も確保できる見通し 資源エネルギー庁',\n",
       " 'article': '資源エネルギー庁は14日午後、記者会見で、北海道では冷え込みが厳しくなる冬に向け、電力需要が高まるものの火力発電所の再稼働などで供給力は確保されるという見通しを示しました。\\n北海道では14日までに京極水力発電所の１号機と２号機が再稼働したことで当面、想定される需要のピーク、383万キロワットを上回る供給力を確保しました。また、今月末以降に苫東厚真火力発電所の１号機が復旧して再稼働すれば、来月前半には合わせて421万キロワットを確保できる見通しだとしています。さらに来月中旬以降に苫東厚真火力発電所の２号機も稼働すれば、来月後半には供給力は481万キロワットまで積み増すことができるとしています。その後も定期検査中の火力発電所を順次、再稼働することで、前の冬のことし１月25日、北海道で需要がピークになった525万キロワットを上回る供給力が確保できる見通しだとしています。しかし、老朽化した火力発電所を稼働させているため、トラブルなどで運転を停止した場合は再び電力が不足する事態に陥りかねないとして、引き続き節電の協力を求めています。',\n",
       " 'genre': ['地震 ライフライン', '北海道地震', '災害'],\n",
       " 'keywords': [],\n",
       " 'url': 'http://www3.nhk.or.jp/news/html/20180914/k10011630731000.html',\n",
       " 'datePublished': '2018-09-14T20:20',\n",
       " 'dateModified': ''}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = driver.current_url\n",
    "html = driver.page_source.encode('utf-8')\n",
    "url_true = 'http' + url.split('/http')[-1]\n",
    "dic = scrape_one_old(html, url_true)\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "\n",
    "js(dic, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
