{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit159cf5e9823545708744fa9c9ccfe5dc",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, json, csv, requests, time, glob, tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rt(text):\n",
    "    return re.sub('<rt>.+?</rt>', '', text)\n",
    "\n",
    "def tag(text):\n",
    "    text = re.sub(r'<span class=\"colorC\">(.+?)</span>', r\"{org}\\1{/org}\", text)\n",
    "    text = re.sub(r'<span class=\"colorL\">(.+?)</span>', r\"{plc}\\1{/plc}\", text)\n",
    "    text = re.sub(r'<span class=\"colorN\">(.+?)</span>', r\"{per}\\1{/per}\", text)\n",
    "    return text\n",
    "\n",
    "def retag(text):\n",
    "    text = re.sub(r'{org}(.+?){/org}', r\"<org>\\1</org>\", text)\n",
    "    text = re.sub(r'{plc}(.+?){/plc}', r\"<plc>\\1</plc>\", text)\n",
    "    text = re.sub(r'{per}(.+?){/per}', r\"<per>\\1</per>\", text)\n",
    "    return text\n",
    "\n",
    "def remove_a(text):\n",
    "    text = re.sub(r'</?a.*?>', '', text)\n",
    "    text = re.sub(r'<span class=\"under\">(\\w+)</span>', r'\\1', text)\n",
    "    text = re.sub(r'<img.+?>(<br ?/?>)?', '', text)\n",
    "    text = re.sub(r'^<br ?/?>', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# for old web easy\n",
    "def scrape_easy_one(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    url_normal = soup.find('div', id=\"regularnews\").a.get('href')\n",
    "    if '/http://' in url_normal:\n",
    "        url_normal = 'http://' + url_normal.split('/http://')[-1]\n",
    "    else:\n",
    "        url_normal = 'https://' + url_normal.split('/https://')[-1]\n",
    "    date = soup.find('p', id=\"newsDate\").text[1:-1]\n",
    "    #title_easy = soup.find('h1', class_=\"article-main__title\")\n",
    "    #title_easy_ruby = ''.join([str(t) for t in title_easy.contents]).strip()\n",
    "    url_easy = soup.find('meta', attrs={'name':'shorturl'}).get('content')\n",
    "    title_easy = soup.find('div', id='newstitle').h2\n",
    "    title_easy_ruby = ''.join([str(t) for t in title_easy.contents]).strip()\n",
    "    title_easy = BeautifulSoup(remove_rt(str(title_easy)), \"html.parser\").text.strip()\n",
    "    article_easy = soup.find('div', id=\"newsarticle\")\n",
    "    article_easy = BeautifulSoup(tag(remove_rt(str(article_easy))), \"html.parser\").text.strip()\n",
    "    article_easy_ruby = soup.find('div', id=\"newsarticle\").find_all('p')\n",
    "    article_easy_ruby = '\\n'.join([''.join([remove_a(str(l)) for l in p.contents]) for p in article_easy_ruby if p != []]).strip()\n",
    "    \n",
    "    return {\n",
    "        'id':url_easy.split('/')[-1].split('.html')[0],\n",
    "        'title_easy':title_easy,\n",
    "        'title_easy_ruby':title_easy_ruby,\n",
    "        'article_easy':retag(article_easy),\n",
    "        'article_easy_ruby':article_easy_ruby,\n",
    "        'url_easy':url_easy,\n",
    "        'url_normal':url_normal,\n",
    "        'date_easy':date\n",
    "    }\n",
    "\n",
    "# for new web easy\n",
    "def scrape_easy_one_new(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    url_easy = 'https://' + soup.find('meta', property=\"og:url\").get('content').split('/https://')[-1]\n",
    "    url_normal = soup.find('div', class_=\"link-to-normal\").a.get('href')\n",
    "    date = soup.find('p', class_=\"article-main__date\").text[1:-1]\n",
    "    title_easy = soup.find('h1', class_=\"article-main__title\")\n",
    "    title_easy_ruby = ''.join([str(t) for t in title_easy.contents]).strip()\n",
    "    title_easy = BeautifulSoup(remove_rt(str(title_easy)), \"html.parser\").text.strip()\n",
    "    article_easy = soup.find('div', class_=\"article-main__body article-body\")\n",
    "    article_easy = BeautifulSoup(tag(remove_rt(str(article_easy))), \"html.parser\").text.strip()\n",
    "    article_easy_ruby = soup.find('div', class_=\"article-main__body article-body\").find_all('p')\n",
    "    article_easy_ruby = '\\n'.join([''.join([remove_a(str(l)) for l in p.contents]) for p in article_easy_ruby if p != []]).strip()\n",
    "    \n",
    "    return {\n",
    "        'id':url_easy.split('/')[-1].split('.html')[0],\n",
    "        'title_easy':title_easy,\n",
    "        'title_easy_ruby':title_easy_ruby,\n",
    "        'article_easy':retag(article_easy),\n",
    "        'article_easy_ruby':article_easy_ruby,\n",
    "        'url_easy':url_easy,\n",
    "        'url_normal':url_normal,\n",
    "        'date_easy':date\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def get_link(start=0):\n",
    "    notyet = []\n",
    "    n_list = pd.read_json('nhkweb.json', encoding='utf-8')['url'].tolist()\n",
    "    df_e = pd.read_json('nhkwebeasy.json', encoding='utf-8') \n",
    "    for i in df_e['url_normal'][start:]:\n",
    "        if i not in n_list:\n",
    "            notyet.append(i)\n",
    "    with open('nolinknormal.txt') as f:\n",
    "        nolink = f.read().split()\n",
    "    return sorted(set(notyet) - set(nolink))\n",
    "        \n",
    "def js_e(dic):\n",
    "    with open('nhkwebeasy.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    with open('nhkwebeasy.json', 'w', encoding='utf-8') as f:\n",
    "        if dic['id'] not in [x['id'] for x in data]:\n",
    "            data.append(dic)\n",
    "        else:\n",
    "            for i, d in enumerate(data):\n",
    "                if dic['id'] == d['id']:\n",
    "                    data[i] = dic\n",
    "        data = sorted(data, key=lambda x:x['id'])\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean category & keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "7821\n4689\n"
    }
   ],
   "source": [
    "with open('nhkweb.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    print(len(data))\n",
    "with open('nhkwebeasy.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "articles:  7821\n"
    },
    {
     "data": {
      "text/plain": "[('社会', 2375),\n ('国際', 2079),\n ('科学・文化', 1268),\n ('ビジネス', 1195),\n ('スポーツ', 980),\n ('政治', 739),\n ('暮らし', 606),\n ('地域', 512),\n ('気象・災害', 354)]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check category\n",
    "\n",
    "with open('nhkweb.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print('articles: ', len(data))\n",
    "genre = Counter()\n",
    "for dic in data:\n",
    "    for g in dic['genre']:\n",
    "        genre[g] += 1\n",
    "genre.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genre <> keywords\n",
    "\n",
    "with open('nhkweb.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "category = ['社会', '国際', 'ビジネス', 'スポーツ', '政治', '科学・文化', '暮らし', '地域', '気象・災害']\n",
    "for i, dic in enumerate(data):\n",
    "    newgenre = []\n",
    "    newkey = []\n",
    "    for j in dic['genre']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        elif j == \"科学・医療\" or j == \"文化・エンタメ\":\n",
    "            newgenre.append('科学・文化')\n",
    "        elif j == \"経済\":\n",
    "            newgenre.append('ビジネス')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    for j in dic['keywords']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    data[i]['genre'] = list(set(newgenre))\n",
    "    data[i]['keywords'] = list(set(newkey))\n",
    "\n",
    "with open('nhkweb.json','w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "k10010353911000    1\nk10010441541000    1\nk10012115201000    1\nk10010055061000    1\nk10010471031000    1\n                  ..\nk10010771931000    1\nk10010399941000    1\nk10010904951000    1\nk10011871991000    1\nk10011099221000    1\nName: id, Length: 4530, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal = pd.read_json('nhkwebeasy.json')\n",
    "normal.id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NHK web easy (new) ID k1001140020 ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1841\n179\n"
    }
   ],
   "source": [
    "with open('tobescraped.txt') as f:\n",
    "    urls = f.read().split()\n",
    "print(len(urls))\n",
    "\n",
    "ids = pd.read_json('nhkwebeasy.json')['id'].tolist()\n",
    "urls = [url for url in urls if url.split('/')[-1].strip('.html') not in ids]\n",
    "print(len(urls))\n",
    "del ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-061aa700613b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_easy_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9af7b82ae48d>\u001b[0m in \u001b[0;36mscrape_easy_one\u001b[0;34m(html)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0murl_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"regularnews\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'/http://'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl_normal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'a'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-061aa700613b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_easy_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_easy_one_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mjs_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9af7b82ae48d>\u001b[0m in \u001b[0;36mscrape_easy_one_new\u001b[0;34m(html)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_easy_one_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0murl_easy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'meta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"og:url\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/https://'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0murl_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"link-to-normal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"article-main__date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "for url in urls[:1000]:\n",
    "    driver.get(url)\n",
    "    time.sleep(6)\n",
    "    html = str(driver.page_source.encode('utf-8'))\n",
    "    snap = re.search(r'(between|1 time).*?<a href=\"(.+?)\">', html)\n",
    "    if snap == None:\n",
    "        continue\n",
    "    driver.get('https://web.archive.org' + snap.group(2))\n",
    "    time.sleep(10)\n",
    "    html = driver.page_source.encode('utf-8')\n",
    "    try:\n",
    "        dic = scrape_easy_one(html)\n",
    "    except:\n",
    "        dic = scrape_easy_one_new(html)\n",
    "    js_e(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': 'k10010179801000',\n 'title_easy': 'インド洋で見つかった翼はマレーシア航空の飛行機',\n 'title_easy_ruby': 'インド<ruby>洋<rt>よう</rt></ruby>で<ruby>見<rt>み</rt></ruby>つかった<ruby>翼<rt>つばさ</rt></ruby>はマレーシア<ruby>航空<rt>こうくう</rt></ruby>の<ruby>飛行機<rt>ひこうき</rt></ruby>',\n 'article_easy': '７月２９日、インド洋にある<plc>フランス</plc>の領地の<plc>レユニオン島</plc>という島の海岸で飛行機の翼が見つかりました。去年３月に南シナ海の空で行方不明になった<org>マレーシア航空</org>の飛行機の可能性があるため、<plc>フランス</plc>で調べていました。\\n<plc>マレーシア</plc>の<per>ナジブ</per>首相は６日、見つかった翼は行方不明の飛行機のものだとわかったと言いました。\\n<plc>フランス</plc>の検察も、見つかった翼は色や形がボーイング７７７という飛行機の翼と同じで、行方不明の飛行機のものだと考えていると言いました。\\n２３９人が乗って行方不明になった飛行機の翼などが見つかったのは初めてです。これから、見つかった翼に爆発の跡がないかどうかなどの調査が行われます。',\n 'article_easy_ruby': '７<ruby>月<rt>がつ</rt></ruby>２９<ruby>日<rt>にち</rt></ruby>、インド<ruby>洋<rt>よう</rt></ruby>にある<span class=\"colorL\">フランス</span>の<ruby>領地<rt>りょうち</rt></ruby>の<span class=\"colorL\">レユニオン<ruby>島<rt>とう</rt></ruby></span>という<ruby>島<rt>しま</rt></ruby>の<ruby>海岸<rt>かいがん</rt></ruby>で<ruby>飛行機<rt>ひこうき</rt></ruby>の<ruby>翼<rt>つばさ</rt></ruby>が<ruby>見<rt>み</rt></ruby>つかりました。<ruby>去年<rt>きょねん</rt></ruby>３<ruby>月<rt>がつ</rt></ruby>に<ruby>南<rt>みなみ</rt></ruby>シナ<ruby>海<rt>かい</rt></ruby>の<ruby>空<rt>そら</rt></ruby>で<ruby>行方不明<rt>ゆくえふめい</rt></ruby>になった<span class=\"colorC\">マレーシア<ruby>航空<rt>こうくう</rt></ruby></span>の<ruby>飛行機<rt>ひこうき</rt></ruby>の<ruby>可能性<rt>かのうせい</rt></ruby>があるため、<span class=\"colorL\">フランス</span>で<ruby>調<rt>しら</rt></ruby>べていました。\\n<span class=\"colorL\">マレーシア</span>の<span class=\"colorN\">ナジブ</span><ruby>首相<rt>しゅしょう</rt></ruby>は<ruby>６日<rt>むいか</rt></ruby>、<ruby>見<rt>み</rt></ruby>つかった<ruby>翼<rt>つばさ</rt></ruby>は<ruby>行方不明<rt>ゆくえふめい</rt></ruby>の<ruby>飛行機<rt>ひこうき</rt></ruby>のものだとわかったと<ruby>言<rt>い</rt></ruby>いました。\\n<span class=\"colorL\">フランス</span>の<ruby>検察<rt>けんさつ</rt></ruby>も、<ruby>見<rt>み</rt></ruby>つかった<ruby>翼<rt>つばさ</rt></ruby>は<ruby>色<rt>いろ</rt></ruby>や<ruby>形<rt>かたち</rt></ruby>がボーイング７７７という<ruby>飛行機<rt>ひこうき</rt></ruby>の<ruby>翼<rt>つばさ</rt></ruby>と<ruby>同<rt>おな</rt></ruby>じで、<ruby>行方不明<rt>ゆくえふめい</rt></ruby>の<ruby>飛行機<rt>ひこうき</rt></ruby>のものだと<ruby>考<rt>かんが</rt></ruby>えていると<ruby>言<rt>い</rt></ruby>いました。\\n２３９<ruby>人<rt>にん</rt></ruby>が<ruby>乗<rt>の</rt></ruby>って<ruby>行方不明<rt>ゆくえふめい</rt></ruby>になった<ruby>飛行機<rt>ひこうき</rt></ruby>の<ruby>翼<rt>つばさ</rt></ruby>などが<ruby>見<rt>み</rt></ruby>つかったのは<ruby>初<rt>はじ</rt></ruby>めてです。これから、<ruby>見<rt>み</rt></ruby>つかった<ruby>翼<rt>つばさ</rt></ruby>に<ruby>爆発<rt>ばくはつ</rt></ruby>の<ruby>跡<rt>あと</rt></ruby>がないかどうかなどの<ruby>調査<rt>ちょうさ</rt></ruby>が<ruby>行<rt>おこな</rt></ruby>われます。',\n 'url_easy': 'http://www3.nhk.or.jp/news/easy/k10010179801000/k10010179801000.html',\n 'url_normal': 'http://www3.nhk.or.jp/news/html/20150806/k10010179801000.html',\n 'date_easy': '08月07日 11時30分'}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = driver.page_source.encode('utf-8')\n",
    "dic = scrape_easy_one(html)\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_e(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<div class=\"link-to-normal\" id=\"js-regular-news-wrapper\">\n<a class=\"btn\" href=\"https://www3.nhk.or.jp/news/html/20200206/k10012274671000.html\" id=\"js-regular-news\" target=\"_blank\"><ruby>普通<rt>ふつう</rt></ruby>のニュースを<ruby>読<rt>よ</rt></ruby>む</a>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "soup.find('div', class_='link-to-normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}