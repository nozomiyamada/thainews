{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bit72de44cd76184052b9457c2863c13ac2",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, json, csv, requests, time, glob, tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rt(text):\n",
    "    return re.sub('<rt>.+?</rt>', '', text)\n",
    "\n",
    "def tag(text):\n",
    "    text = re.sub(r'<span class=\"colorC\">(.+?)</span>', r\"{org}\\1{/org}\", text)\n",
    "    text = re.sub(r'<span class=\"colorL\">(.+?)</span>', r\"{plc}\\1{/plc}\", text)\n",
    "    text = re.sub(r'<span class=\"colorN\">(.+?)</span>', r\"{per}\\1{/per}\", text)\n",
    "    return text\n",
    "\n",
    "def retag(text):\n",
    "    text = re.sub(r'{org}(.+?){/org}', r\"<org>\\1</org>\", text)\n",
    "    text = re.sub(r'{plc}(.+?){/plc}', r\"<plc>\\1</plc>\", text)\n",
    "    text = re.sub(r'{per}(.+?){/per}', r\"<per>\\1</per>\", text)\n",
    "    return text\n",
    "\n",
    "def remove_a(text):\n",
    "    text = re.sub(r'</?a.*?>', '', text)\n",
    "    text = re.sub(r'<span class=\"under\">(\\w+)</span>', r'\\1', text)\n",
    "    text = re.sub(r'<img.+?>(<br ?/?>)?', '', text)\n",
    "    text = re.sub(r'^<br ?/?>', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# for old web easy\n",
    "def scrape_easy_one(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    url_normal = soup.find('div', id=\"regularnews\")\n",
    "    if url_normal == None:\n",
    "        url_normal = soup.find('p', id=\"regularnews\")\n",
    "    if url_normal == None:\n",
    "        url_normal = \"\"\n",
    "    else:\n",
    "        url_normal = url_normal.a.get('href')\n",
    "        if '/http://' in url_normal:\n",
    "            url_normal = 'http://' + url_normal.split('/http://')[-1]\n",
    "        else:\n",
    "            url_normal = 'https://' + url_normal.split('/https://')[-1]\n",
    "    date = soup.find('p', id=\"newsDate\").text[1:-1]\n",
    "    url_easy = soup.find('meta', attrs={'name':'shorturl'}).get('content')\n",
    "    title_easy = soup.find('div', id='newstitle').h2\n",
    "    title_easy_ruby = ''.join([str(t) for t in title_easy.contents]).strip()\n",
    "    title_easy = BeautifulSoup(remove_rt(str(title_easy)), \"html.parser\").text.strip()\n",
    "    article_easy = soup.find('div', id=\"newsarticle\")\n",
    "    article_easy = BeautifulSoup(tag(remove_rt(str(article_easy))), \"html.parser\").text.strip()\n",
    "    article_easy_ruby = soup.find('div', id=\"newsarticle\").find_all('p')\n",
    "    article_easy_ruby = '\\n'.join([''.join([remove_a(str(l)) for l in p.contents]) for p in article_easy_ruby if p != []]).strip()\n",
    "    \n",
    "    return {\n",
    "        'id':url_easy.split('/')[-1].split('.html')[0],\n",
    "        'title_easy':title_easy,\n",
    "        'title_easy_ruby':title_easy_ruby,\n",
    "        'article_easy':retag(article_easy),\n",
    "        'article_easy_ruby':article_easy_ruby,\n",
    "        'url_easy':url_easy,\n",
    "        'url_normal':url_normal,\n",
    "        'date_easy':date\n",
    "    }\n",
    "\n",
    "# for new web easy\n",
    "def scrape_easy_one_new(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    url_easy = 'https://' + soup.find('meta', property=\"og:url\").get('content').split('/https://')[-1]\n",
    "    url_normal = soup.find('div', class_=\"link-to-normal\").a.get('href')\n",
    "    date = soup.find('p', class_=\"article-main__date\").text[1:-1]\n",
    "    title_easy = soup.find('h1', class_=\"article-main__title\")\n",
    "    title_easy_ruby = ''.join([str(t) for t in title_easy.contents]).strip()\n",
    "    title_easy = BeautifulSoup(remove_rt(str(title_easy)), \"html.parser\").text.strip()\n",
    "    article_easy = soup.find('div', class_=\"article-main__body article-body\")\n",
    "    article_easy = BeautifulSoup(tag(remove_rt(str(article_easy))), \"html.parser\").text.strip()\n",
    "    article_easy_ruby = soup.find('div', class_=\"article-main__body article-body\").find_all('p')\n",
    "    article_easy_ruby = '\\n'.join([''.join([remove_a(str(l)) for l in p.contents]) for p in article_easy_ruby if p != []]).strip()\n",
    "    \n",
    "    return {\n",
    "        'id':url_easy.split('/')[-1].split('.html')[0],\n",
    "        'title_easy':title_easy,\n",
    "        'title_easy_ruby':title_easy_ruby,\n",
    "        'article_easy':retag(article_easy),\n",
    "        'article_easy_ruby':article_easy_ruby,\n",
    "        'url_easy':url_easy,\n",
    "        'url_normal':url_normal,\n",
    "        'date_easy':date\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def get_link(start=0):\n",
    "    notyet = []\n",
    "    n_list = pd.read_json('nhkweb.json', encoding='utf-8')['url'].tolist()\n",
    "    df_e = pd.read_json('nhkwebeasy.json', encoding='utf-8') \n",
    "    for i in df_e['url_normal'][start:]:\n",
    "        if i not in n_list:\n",
    "            notyet.append(i)\n",
    "    with open('nolinknormal.txt') as f:\n",
    "        nolink = f.read().split()\n",
    "    return sorted(set(notyet) - set(nolink))\n",
    "        \n",
    "def js_e(dic):\n",
    "    with open('nhkwebeasy.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    with open('nhkwebeasy.json', 'w', encoding='utf-8') as f:\n",
    "        if dic['id'] not in [x['id'] for x in data]:\n",
    "            data.append(dic)\n",
    "        else:\n",
    "            for i, d in enumerate(data):\n",
    "                if dic['id'] == d['id']:\n",
    "                    data[i] = dic\n",
    "        data = sorted(data, key=lambda x:x['id'])\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nhkweb.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    print(len(data))\n",
    "with open('nhkwebeasy.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check category\n",
    "\n",
    "with open('nhkweb.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print('articles: ', len(data))\n",
    "genre = Counter()\n",
    "for dic in data:\n",
    "    for g in dic['genre']:\n",
    "        genre[g] += 1\n",
    "genre.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genre <> keywords\n",
    "\n",
    "with open('nhkweb.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "category = ['社会', '国際', 'ビジネス', 'スポーツ', '政治', '科学・文化', '暮らし', '地域', '気象・災害']\n",
    "for i, dic in enumerate(data):\n",
    "    newgenre = []\n",
    "    newkey = []\n",
    "    for j in dic['genre']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        elif j == \"科学・医療\" or j == \"文化・エンタメ\":\n",
    "            newgenre.append('科学・文化')\n",
    "        elif j == \"経済\":\n",
    "            newgenre.append('ビジネス')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    for j in dic['keywords']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    data[i]['genre'] = list(set(newgenre))\n",
    "    data[i]['keywords'] = list(set(newkey))\n",
    "\n",
    "with open('nhkweb.json','w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NHK web easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "918 5742\nnot yet 0\n"
    },
    {
     "data": {
      "text/plain": "['https://web.archive.org/web/*/http://www3.nhk.or.jp/news/easy/k10014345501000/k10014345501000.html',\n 'https://web.archive.org/web/*/http://www3.nhk.or.jp/news/easy/k10014358721000/k10014358721000.html',\n 'https://web.archive.org/web/*/http://www3.nhk.or.jp/news/easy/k10014360391000/k10014360391000.html',\n 'https://web.archive.org/web/*/http://www3.nhk.or.jp/news/easy/k10014364701000/k10014364701000.html',\n 'https://web.archive.org/web/*/http://www3.nhk.or.jp/news/easy/k10014407931000/k10014407931000.html',\n 'https://web.archive.org/web/*/http://www3.nhk.or.jp/news/easy/k10014420291000/k10014420291000.html',\n 'https://web.archive.org/web/*/http://www3.nhk.or.jp/news/easy/k10014425011000/k10014425011000.html',\n 'https://web.archive.org/web/*/http://www3.nhk.or.jp/news/easy/k10014425961000/k10014425961000.html',\n 'https://web.archive.org/web/*/http://www3.nhk.or.jp/news/easy/k10014450891000/k10014450891000.html',\n 'https://web.archive.org/web/*/http://www3.nhk.or.jp/news/easy/k10014466031000/k10014466031000.html']"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('linkeasy.txt') as f:\n",
    "    urls = f.read().strip().split()\n",
    "    ids = set([x.split('.html')[0].split('/')[-1] for x in urls])\n",
    "\n",
    "id_exist = set(pd.read_json('nhkwebeasy.json')['id'].tolist())\n",
    "print(len(ids), len(id_exist))\n",
    "print('not yet', len(ids - id_exist))\n",
    "urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_exist = set(pd.read_json('nhkwebeasy.json')['id'].tolist())\n",
    "for url in urls:\n",
    "    ID = url.split('.html')[0].split('/')[-1]\n",
    "    if ID in id_exist:\n",
    "        continue\n",
    "    driver.get(url)\n",
    "    time.sleep(7)\n",
    "    html = str(driver.page_source.encode('utf-8'))\n",
    "    snap = re.search(r'(between|1 time).*?<a href=\"(.+?)\">', html)\n",
    "    archiveurl = 'https://web.archive.org' + snap.group(2)\n",
    "    response = requests.get(archiveurl)\n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "    elif response.status_code == 504:\n",
    "        response = requests.get(archiveurl)\n",
    "        if response.status_code == 504:\n",
    "            raise AssertionError\n",
    "        html = response.text\n",
    "    time.sleep(3)\n",
    "    \"\"\"\n",
    "    driver.get(archiveurl)\n",
    "    time.sleep(10)\n",
    "    html = driver.page_source.encode('utf-8')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dic = scrape_easy_one(html)\n",
    "    except:\n",
    "        dic = scrape_easy_one_new(html)\n",
    "    js_e(dic)\n",
    "    id_exist = set(pd.read_json('nhkwebeasy.json')['id'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': 'k10015884001000',\n 'title_easy': '楽天の優勝セール\\u3000安く売っているように見せた疑い',\n 'title_easy_ruby': '<ruby>楽天<rt>らくてん</rt></ruby>の<ruby>優勝<rt>ゆうしょう</rt></ruby>セール\\u3000<ruby>安<rt>やす</rt></ruby>く<ruby>売<rt>う</rt></ruby>っているように<ruby>見<rt>み</rt></ruby>せた<ruby>疑<rt>うたが</rt></ruby>い',\n 'article_easy': 'ことしのプロ野球は<org>東北楽天イーグルス</org>が優勝しました。このチームを持っている会社の<org>楽天</org>は、いろいろな店を集めた「楽天市場」というウェブサイトを運営しています。「楽天市場」ではすぐに優勝セールを始めました。その中には７７％安くして売っている品物もありました。\\nしかし、安くなる前の値段をいつもの値段より高く書いて、安くした割合を大きく見せている店があるとインターネットを見た人たちが言ってきました。<org>楽天</org>が調べると、今までに約２０の店が約１０００の品物を安くなったように見せて売っていた疑いがあることが分かりました。\\n<org>楽天</org>によると、今回のセールは<org>楽天</org>がよく調べてからインターネットに品物を出すように決めています。しかし、問題になった店は<org>楽天</org>が調べていないのに、インターネットに品物を出していました。このため<org>楽天</org>は、今回問題になった品物を買った客にはお金を返すなどするように店に言うことにしています。',\n 'article_easy_ruby': 'ことしのプロ<ruby>野球<rt>やきゅう</rt></ruby>は<span class=\"colorC\"><ruby>東北<rt>とうほく</rt></ruby><ruby>楽天<rt>らくてん</rt></ruby>イーグルス</span>が<ruby>優勝<rt>ゆうしょう</rt></ruby>しました。このチームを<ruby>持<rt>も</rt></ruby>っている<ruby>会社<rt>かいしゃ</rt></ruby>の<span class=\"colorC\"><ruby>楽天<rt>らくてん</rt></ruby></span>は、いろいろな<ruby>店<rt>みせ</rt></ruby>を<ruby>集<rt>あつ</rt></ruby>めた「<ruby>楽天<rt>らくてん</rt></ruby><ruby>市場<rt>いちば</rt></ruby>」というウェブサイトを<ruby>運営<rt>うんえい</rt></ruby>しています。「<ruby>楽天<rt>らくてん</rt></ruby><ruby>市場<rt>いちば</rt></ruby>」ではすぐに<ruby>優勝<rt>ゆうしょう</rt></ruby>セールを<ruby>始<rt>はじ</rt></ruby>めました。その<ruby>中<rt>なか</rt></ruby>には７７％<ruby>安<rt>やす</rt></ruby>くして<ruby>売<rt>う</rt></ruby>っている<ruby>品物<rt>しなもの</rt></ruby>もありました。\\nしかし、<ruby>安<rt>やす</rt></ruby>くなる<ruby>前<rt>まえ</rt></ruby>の<ruby>値段<rt>ねだん</rt></ruby>をいつもの<ruby>値段<rt>ねだん</rt></ruby>より<ruby>高<rt>たか</rt></ruby>く<ruby>書<rt>か</rt></ruby>いて、<ruby>安<rt>やす</rt></ruby>くした<ruby>割合<rt>わりあい</rt></ruby>を<ruby>大<rt>おお</rt></ruby>きく<ruby>見<rt>み</rt></ruby>せている<ruby>店<rt>みせ</rt></ruby>があるとインターネットを<ruby>見<rt>み</rt></ruby>た<ruby>人<rt>ひと</rt></ruby>たちが<ruby>言<rt>い</rt></ruby>ってきました。<span class=\"colorC\"><ruby>楽天<rt>らくてん</rt></ruby></span>が<ruby>調<rt>しら</rt></ruby>べると、<ruby>今<rt>いま</rt></ruby>までに<ruby>約<rt>やく</rt></ruby>２０の<ruby>店<rt>みせ</rt></ruby>が<ruby>約<rt>やく</rt></ruby>１０００の<ruby>品物<rt>しなもの</rt></ruby>を<ruby>安<rt>やす</rt></ruby>くなったように<ruby>見<rt>み</rt></ruby>せて<ruby>売<rt>う</rt></ruby>っていた<ruby>疑<rt>うたが</rt></ruby>いがあることが<ruby>分<rt>わ</rt></ruby>かりました。\\n<span class=\"colorC\"><ruby>楽天<rt>らくてん</rt></ruby></span>によると、<ruby>今回<rt>こんかい</rt></ruby>のセールは<span class=\"colorC\"><ruby>楽天<rt>らくてん</rt></ruby></span>がよく<ruby>調<rt>しら</rt></ruby>べてからインターネットに<ruby>品物<rt>しなもの</rt></ruby>を<ruby>出<rt>だ</rt></ruby>すように<ruby>決<rt>き</rt></ruby>めています。しかし、<ruby>問題<rt>もんだい</rt></ruby>になった<ruby>店<rt>みせ</rt></ruby>は<span class=\"colorC\"><ruby>楽天<rt>らくてん</rt></ruby></span>が<ruby>調<rt>しら</rt></ruby>べていないのに、インターネットに<ruby>品物<rt>しなもの</rt></ruby>を<ruby>出<rt>だ</rt></ruby>していました。このため<span class=\"colorC\"><ruby>楽天<rt>らくてん</rt></ruby></span>は、<ruby>今回<rt>こんかい</rt></ruby><ruby>問題<rt>もんだい</rt></ruby>になった<ruby>品物<rt>しなもの</rt></ruby>を<ruby>買<rt>か</rt></ruby>った<ruby>客<rt>きゃく</rt></ruby>にはお<ruby>金<rt>かね</rt></ruby>を<ruby>返<rt>かえ</rt></ruby>すなどするように<ruby>店<rt>みせ</rt></ruby>に<ruby>言<rt>い</rt></ruby>うことにしています。',\n 'url_easy': 'http://www3.nhk.or.jp/news/easy/k10015884001000/k10015884001000.html',\n 'url_normal': 'http://www3.nhk.or.jp/news/html/20131107/k10015884001000.html',\n 'date_easy': '11月08日 16時00分'}"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = driver.page_source.encode('utf-8')\n",
    "dic = scrape_easy_one(html)\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_e(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "articles:  8876\n"
    },
    {
     "data": {
      "text/plain": "[('社会', 2644),\n ('国際', 2383),\n ('科学・文化', 1484),\n ('ビジネス', 1369),\n ('スポーツ', 1075),\n ('政治', 775),\n ('暮らし', 732),\n ('地域', 615),\n ('気象・災害', 414)]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check category\n",
    "\n",
    "with open('nhkweb.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print('articles: ', len(data))\n",
    "genre = Counter()\n",
    "for dic in data:\n",
    "    for g in dic['genre']:\n",
    "        genre[g] += 1\n",
    "genre.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genre <> keywords\n",
    "with open('nhkweb.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "category = ['社会', '国際', 'ビジネス', 'スポーツ', '政治', '科学・文化', '暮らし', '地域', '気象・災害']\n",
    "for i, dic in enumerate(data):\n",
    "    newgenre = []\n",
    "    newkey = []\n",
    "    for j in dic['genre']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        elif j == \"科学・医療\" or j == \"文化・エンタメ\":\n",
    "            newgenre.append('科学・文化')\n",
    "        elif j == \"経済\":\n",
    "            newgenre.append('ビジネス')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    for j in dic['keywords']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    data[i]['genre'] = list(set(newgenre))\n",
    "    data[i]['keywords'] = list(set(newkey))\n",
    "\n",
    "with open('nhkweb.json','w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}