{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bit72de44cd76184052b9457c2863c13ac2",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, json, csv, requests, time, glob, tqdm, datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_one_new(html, url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    json_data = json.loads(soup.find_all(\"script\", type=\"application/ld+json\")[-1].text)\n",
    "    title = json_data.get('headline', soup.find('span', class_='contentTitle').text)\n",
    "    date = json_data.get('datePublished', re.search(r'datetime:.*?(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2})', str(html)).group(1))\n",
    "    date_m = json_data.get('dateModified', '')\n",
    "    genre = json_data.get('genre', [])\n",
    "    if genre == []:\n",
    "        genre = [k for k in soup.find('meta', attrs={'name':'keywords'}).get('content').split(',') if k not in ['NHK','ニュース', 'NHK NEWS WEB']]\n",
    "    keywords = json_data.get('keywords', [])\n",
    "    article = soup.find('div', id=\"news_textbody\").text\n",
    "    if soup.find_all('div', id=\"news_textmore\") != []:\n",
    "        for textmore in soup.find_all('div', id=\"news_textmore\"):\n",
    "            article += ('\\n' + textmore.text)\n",
    "    if soup.find_all('div', class_=\"news_add\") != []:\n",
    "        for newsadd in soup.find_all('div', class_=\"news_add\"):\n",
    "            if newsadd.h3 != None:\n",
    "                newsadd.h3.extract()\n",
    "            article += ('\\n' + newsadd.text)\n",
    "            \n",
    "    return {\n",
    "        'id':url.split('/')[-1].split('.html')[0],\n",
    "        'title':title,\n",
    "        'article':article.strip(),\n",
    "        'genre':genre,\n",
    "        'keywords':keywords,\n",
    "        'url':url,\n",
    "        'datePublished':date,\n",
    "        'dateModified':date_m\n",
    "    }\n",
    "\n",
    "# for old web normal\n",
    "def make_date_normal_old(ymd,time):\n",
    "    year, month, day = ymd[:4], ymd[4:6], ymd[6:]\n",
    "    hour, minute = time.split('時')\n",
    "    minute = minute.strip('分')\n",
    "    if len(hour) == 1:\n",
    "        hour = '0' + hour\n",
    "    if len(minute) == 1:\n",
    "        minute = '0' + minute\n",
    "    return f\"{year}-{month}-{day}T{hour}:{minute}\"\n",
    "\n",
    "def scrape_one_old(html, url_true):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    title = soup.find('span', class_=\"contentTitle\").text.strip()\n",
    "    ymd_ = url_true.split('/')[-2]\n",
    "    time_ = soup.find('span', id=\"news_time\").text\n",
    "    date = make_date_normal_old(ymd_, time_)\n",
    "    genre = [k for k in soup.find('meta', attrs={'name':'keywords'}).get('content').split(',') if k not in ['NHK','ニュース', 'NHK NEWS WEB','ＮＨＫ','ＮＨＫニュース','']]\n",
    "    article = soup.find(['div','p'], id=\"news_textbody\").text\n",
    "    if soup.find_all(['div','p'], id=\"news_textmore\") != []:\n",
    "        for textmore in soup.find_all(['div','p'], id=\"news_textmore\"):\n",
    "            article += ('\\n' + textmore.text)\n",
    "    if soup.find_all(['div','p'], class_=\"news_add\") != []:\n",
    "        for newsadd in soup.find_all(['div','p'], class_=\"news_add\"):\n",
    "            if newsadd.h3 != None:\n",
    "                newsadd.h3.extract()\n",
    "            article += ('\\n' + newsadd.text)\n",
    "            \n",
    "    return {\n",
    "        'id':url_true.split('/')[-1].split('.html')[0],\n",
    "        'title':title,\n",
    "        'article':article.strip(),\n",
    "        'genre':genre,\n",
    "        'keywords':[],\n",
    "        'url':url_true,\n",
    "        'datePublished':date,\n",
    "        'dateModified':\"\"\n",
    "    }\n",
    "\n",
    "def write_nolink(ID, year):\n",
    "    with open(f'nolinknormal{year}.txt', 'a') as f:\n",
    "        f.write(str(ID) + '\\n')\n",
    "\n",
    "def get_archiveurl_from_id(ID, date, http=True):\n",
    "    if http:\n",
    "        url = f'https://web.archive.org/web/*/http://www3.nhk.or.jp/news/html/{date}/k{ID}1000.html'\n",
    "    else:\n",
    "        url = f'https://web.archive.org/web/*/https://www3.nhk.or.jp/news/html/{date}/k{ID}1000.html'\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    html = str(driver.page_source.encode('utf-8'))\n",
    "    snap = re.search(r'(times between|1 time|times).*?<a href=\"(.+?)\">', html)\n",
    "    archiveurl = 'https://web.archive.org' + snap.group(2)\n",
    "    return None if 'nhk' not in archiveurl else archiveurl\n",
    "\n",
    "def get_article_from_archiveurl(archiveurl, browser=True):\n",
    "    if browser == False:\n",
    "        response = requests.get(archiveurl)\n",
    "        html = response.text\n",
    "    else:\n",
    "        driver.get(archiveurl)\n",
    "        time.sleep(10)\n",
    "        html = driver.page_source.encode('utf-8')\n",
    "\n",
    "    url_true = 'http' + archiveurl.split('/http')[-1]\n",
    "    if 'This page is not available on the web' in str(html):\n",
    "        return None\n",
    "    try:\n",
    "        try:\n",
    "            dic = scrape_one_old(html, url_true)\n",
    "        except:\n",
    "            dic = scrape_one_new(html, url_true)\n",
    "        return dic\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def js(dic, year):\n",
    "    if dic == None:\n",
    "        return\n",
    "    with open(f'nhkweb{year}.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    with open(f'nhkweb{year}.json', 'w', encoding='utf-8') as f:\n",
    "        if dic['id'] not in [x['id'] for x in data]:\n",
    "            data.append(dic)\n",
    "        else:\n",
    "            for i, d in enumerate(data):\n",
    "                if dic['id'] == d['id']:\n",
    "                    data[i] = dic\n",
    "        data = sorted(data, key=lambda x:x['id'])\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def datebefore(ymd:str):\n",
    "    year, month, day = int(ymd[:4]), int(ymd[4:6]), int(ymd[6:])\n",
    "    before = str(datetime.datetime(year, month, day) + datetime.timedelta(-1))\n",
    "    year, month, day = before[:4], before[5:7], before[8:10]\n",
    "    return f'{year}{month}{day}'\n",
    "\n",
    "def dateafter(ymd:str):\n",
    "    year, month, day = int(ymd[:4]), int(ymd[4:6]), int(ymd[6:])\n",
    "    before = str(datetime.datetime(year, month, day) + datetime.timedelta(1))\n",
    "    year, month, day = before[:4], before[5:7], before[8:10]\n",
    "    return f'{year}{month}{day}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "42357"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "len(pd.read_json('nhkweb2019.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get missing url & article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "21920\n           id                                                url      date\n0  1001127630  http://www3.nhk.or.jp/news/html/20180101/k1001...  20180101\n1  1001127631  http://www3.nhk.or.jp/news/html/20180101/k1001...  20180101\n2  1001127632  http://www3.nhk.or.jp/news/html/20171231/k1001...  20171231\n3  1001127633  http://www3.nhk.or.jp/news/html/20171231/k1001...  20171231\n4  1001127634  http://www3.nhk.or.jp/news/html/20171231/k1001...  20171231\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               id                                                url      date\n14105  1001143171  http://www3.nhk.or.jp/news/html/20180509/k1001...  20180509",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14105</th>\n      <td>1001143171</td>\n      <td>http://www3.nhk.or.jp/news/html/20180509/k1001...</td>\n      <td>20180509</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "data = pd.read_json('nhkweb2018.json')\n",
    "data = data[['id', 'url']]\n",
    "data.id = data.id.apply(lambda x:x[1:-4])\n",
    "data['date'] = data.url.apply(lambda x:x.split('news/html/')[-1].split('/')[0])\n",
    "print(len(data))\n",
    "print(data.head())\n",
    "data[data.id == \"1001143171\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "#options.headless = True\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "\n  0%|          | 0/7374 [00:00<?, ?it/s]\u001b[A\n  0%|          | 8/7374 [00:21<5:30:51,  2.70s/it]\u001b[A\n  0%|          | 9/7374 [17:41<641:53:10, 313.75s/it]\u001b[A\n  0%|          | 10/7374 [17:59<460:19:35, 225.04s/it]\u001b[A\n  0%|          | 12/7374 [20:35<370:06:15, 180.98s/it]\u001b[A\n  0%|          | 14/7374 [21:11<269:58:02, 132.05s/it]\u001b[A\n  0%|          | 16/7374 [22:54<220:33:08, 107.91s/it]\u001b[A\n  0%|          | 17/7374 [23:23<171:58:01, 84.15s/it]\u001b[A\n  0%|          | 22/7374 [24:29<128:24:20, 62.88s/it]\u001b[A\n  0%|          | 28/7374 [24:58<92:49:08, 45.49s/it]\u001b[A\n  0%|          | 29/7374 [25:17<76:06:04, 37.30s/it]\u001b[A\n  0%|          | 35/7374 [25:54<57:02:20, 27.98s/it]\u001b[A\n  0%|          | 36/7374 [26:34<64:11:36, 31.49s/it]\u001b[A\n  1%|          | 37/7374 [44:38<707:49:17, 347.30s/it]\u001b[A\n  1%|          | 39/7374 [46:15<525:02:21, 257.69s/it]\u001b[A\n  1%|          | 40/7374 [48:45<459:15:44, 225.44s/it]\u001b[A\n  1%|          | 47/7374 [49:25<324:37:05, 159.50s/it]\u001b[A\n  1%|          | 48/7374 [50:32<268:31:09, 131.95s/it]\u001b[A\n  1%|          | 50/7374 [53:39<244:46:04, 120.31s/it]\u001b[A\n  1%|          | 52/7374 [54:14<181:57:00, 89.46s/it]\u001b[A\n  1%|          | 53/7374 [54:46<147:09:55, 72.37s/it]\u001b[A\n  1%|          | 55/7374 [55:21<113:37:57, 55.89s/it]\u001b[A\n  1%|          | 56/7374 [55:58<101:55:52, 50.14s/it]\u001b[A\n  1%|          | 70/7374 [1:15:37<122:30:35, 60.38s/it]\u001b[A\n  1%|          | 72/7374 [1:16:19<98:26:17, 48.53s/it]\u001b[A\n  1%|          | 73/7374 [1:19:41<191:33:00, 94.45s/it]\u001b[A\n  1%|          | 75/7374 [1:20:08<142:16:57, 70.18s/it]\u001b[A\n  1%|          | 77/7374 [1:20:41<109:37:20, 54.08s/it]\u001b[A\n  1%|          | 84/7374 [1:22:17<84:57:18, 41.95s/it]\u001b[A\n  1%|          | 85/7374 [1:23:30<104:05:47, 51.41s/it]\u001b[A\n  1%|▏         | 97/7374 [1:24:12<74:50:51, 37.03s/it]\u001b[A\n  1%|▏         | 102/7374 [1:33:08<117:22:59, 58.11s/it]\u001b[A\n  1%|▏         | 103/7374 [1:39:14<303:40:04, 150.35s/it]\u001b[A\n  1%|▏         | 105/7374 [1:39:55<225:01:04, 111.44s/it]\u001b[A\n  1%|▏         | 107/7374 [1:40:33<169:07:49, 83.79s/it]\u001b[A\n  2%|▏         | 112/7374 [1:41:27<124:48:25, 61.87s/it]\u001b[A\n  2%|▏         | 113/7374 [1:42:19<118:44:25, 58.87s/it]\u001b[A\n  2%|▏         | 119/7374 [1:43:28<89:56:59, 44.63s/it]\u001b[A\n  2%|▏         | 120/7374 [1:43:46<74:21:00, 36.90s/it]\u001b[A\n  2%|▏         | 124/7374 [1:44:06<54:54:45, 27.27s/it]\u001b[A\n  2%|▏         | 125/7374 [1:46:04<110:16:26, 54.76s/it]\u001b[A\n  2%|▏         | 126/7374 [1:47:37<132:48:12, 65.96s/it]\u001b[A\n  2%|▏         | 127/7374 [1:47:57<105:18:46, 52.32s/it]\u001b[A\n  2%|▏         | 130/7374 [1:49:24<91:12:55, 45.33s/it]\u001b[A\n  2%|▏         | 134/7374 [2:32:17<451:57:42, 224.73s/it]\u001b[A\n  2%|▏         | 136/7374 [2:33:46<342:57:30, 170.58s/it]\u001b[A\n  2%|▏         | 140/7374 [2:34:46<248:58:19, 123.90s/it]\u001b[A\n  2%|▏         | 142/7374 [2:35:09<181:13:13, 90.21s/it]\u001b[A\n  2%|▏         | 143/7374 [2:36:03<159:16:45, 79.30s/it]\u001b[A\n  2%|▏         | 164/7374 [2:37:01<112:50:16, 56.34s/it]\u001b[A\n  2%|▏         | 165/7374 [2:38:03<115:56:27, 57.90s/it]\u001b[A\n  2%|▏         | 166/7374 [3:44:10<2463:59:32, 1230.63s/it]\u001b[A\n  2%|▏         | 169/7374 [3:45:02<1734:37:24, 866.71s/it]\u001b[A\n  2%|▏         | 173/7374 [3:46:42<1228:26:38, 614.14s/it]\u001b[A\n  2%|▏         | 179/7374 [3:47:56<866:35:38, 433.60s/it]\u001b[A\n  2%|▏         | 180/7374 [3:49:15<653:56:53, 327.25s/it]\u001b[A\n  3%|▎         | 185/7374 [3:53:00<484:28:20, 242.61s/it]\u001b[A\n  3%|▎         | 192/7374 [3:54:10<344:48:12, 172.83s/it]\u001b[A\n  3%|▎         | 193/7374 [3:57:10<348:30:25, 174.71s/it]\u001b[A\n  3%|▎         | 194/7374 [3:59:37<332:12:06, 166.56s/it]\u001b[A\n  3%|▎         | 196/7374 [4:00:33<249:17:15, 125.03s/it]\u001b[A"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a5d599c08845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0marchiveurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_archiveurl_from_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# try the same day\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marchiveurl\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_article_from_archiveurl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchiveurl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7188cfb50cfa>\u001b[0m in \u001b[0;36mget_archiveurl_from_id\u001b[0;34m(ID, date, http)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0msnap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(times between|1 time|times).*?<a href=\"(.+?)\">'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0marchiveurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://web.archive.org'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msnap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'nhk'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marchiveurl\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marchiveurl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "year = 2018\n",
    "\n",
    "for i in tqdm.tqdm(range(14105, 21000)):\n",
    "    # get the present row & next row\n",
    "    ID1, ID2 = data.iat[i,0], data.iat[i+1,0]\n",
    "    date = data.iat[i,2]\n",
    "    before = datebefore(date)\n",
    "    after = dateafter(date)\n",
    "\n",
    "    # ID is continuous = no missing\n",
    "    if int(ID1) + 1 == int(ID2):\n",
    "        continue\n",
    "\n",
    "    # not continuous\n",
    "    else:  \n",
    "        for ID in range(int(ID1)+1, int(ID2)):\n",
    "            archiveurl = get_archiveurl_from_id(ID, date) # try the same day\n",
    "            if archiveurl != None:\n",
    "                dic = get_article_from_archiveurl(archiveurl)\n",
    "                if dic:\n",
    "                    js(dic, year)\n",
    "                else:\n",
    "                    write_nolink(ID, year) # error in NHK\n",
    "            else:\n",
    "                archiveurl = get_archiveurl_from_id(ID, after) # try the next day\n",
    "                if archiveurl != None:\n",
    "                    dic = get_article_from_archiveurl(archiveurl)\n",
    "                    if dic:\n",
    "                        js(dic, year)\n",
    "                    else:\n",
    "                        write_nolink(ID, year) # error in NHK\n",
    "                else: \n",
    "                    archiveurl = get_archiveurl_from_id(ID, before) # try the previous day\n",
    "                    if archiveurl != None:\n",
    "                        dic = get_article_from_archiveurl(archiveurl)\n",
    "                        if dic:\n",
    "                            js(dic, year)\n",
    "                        else:\n",
    "                            write_nolink(ID, year) # error in NHK\n",
    "                    else:\n",
    "                        write_nolink(ID, year) # nolink\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'1001143171'"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "ID1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n  \n"
    }
   ],
   "source": [
    "# delete fduplicate\n",
    "\n",
    "nolinklist = pd.read_csv('nolinknormal.txt', header=None)\n",
    "pd.DataFrame(sorted(set(map(str, nolinklist[0])) - set(data.id)))[0].to_csv('nolinknormal.txt', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'https://web.archive.org/web/20180401030106/http://www3.nhk.or.jp/news/html/20180331/k10011386861000.html'"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "archiveurl = get_archiveurl_from_id(1001138686, 20180331)\n",
    "archiveurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "''"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "response = requests.get(archiveurl)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c4eebf963495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscrape_one_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchiveurl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-aae2be1a8424>\u001b[0m in \u001b[0;36mscrape_one_old\u001b[0;34m(html, url)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_one_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"contentTitle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mhmd_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtime_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"news_time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "scrape_one_old(html, archiveurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'id': 'k10011278991000',\n 'title': '「北朝鮮は脅威」米でも８割 日米世論調査',\n 'article': 'アメリカでトランプ政権が発足してから今月で１年を迎えるのを前に、ＮＨＫが日本とアメリカで実施した世論調査で、北朝鮮の核やミサイルの問題について「脅威だ」と答えた人がアメリカでも８割に上り、北朝鮮の脅威を現実のものとして国民が受け止めていることが明らかになりました。\\nＮＨＫは、去年１１月２７日から１２月３日にかけて、日本とアメリカの１８歳以上の男女を対象に、コンピューターで無作為に発生させた固定電話と携帯電話の番号に電話をかける「ＲＤＤ」という方法で世論調査を行いました。調査では、日本で１２３２人、アメリカでは１２０１人から回答を得ました。\\n\\n\\nＮＨＫは、去年１１月２７日から１２月３日にかけて、日本とアメリカの１８歳以上の男女を対象に、コンピューターで無作為に発生させた固定電話と携帯電話の番号に電話をかける「ＲＤＤ」という方法で世論調査を行いました。調査では、日本で１２３２人、アメリカでは１２０１人から回答を得ました。\\n\\n\\n\\n\\nこの中で、北朝鮮の核やミサイルの問題はどの程度の脅威であると思うか聞いたところ、日本では「非常に脅威だ」が４８％、「ある程度脅威だ」が３３％と、脅威と感じる人は合わせて８１％でした。一方、アメリカでは「非常に脅威だ」が５０％、「ある程度脅威だ」が３３％と脅威と感じる人が合わせて８３％で、北朝鮮がアメリカ本土全域を攻撃できるＩＣＢＭ＝大陸間弾道ミサイルの発射実験に成功したと主張する中、アメリカでも国民が北朝鮮の脅威を現実のものとして受け止めていることが明らかになりました。\\n\\n\\n\\n\\nさらに、北朝鮮の核やミサイルの問題を解決するためには何が最も有効だと思うか聞いたところ、アメリカでは、「話し合い」が最も多く３６％、次いで、「経済的圧力」が２４％、「軍事行動」が１８％、「軍事的圧力」が１７％という結果になりました。一方、日本では、最も多かったのが「経済的圧力」で３５％、次いで、「話し合い」が３１％、「軍事的圧力」が１５％、「軍事行動」が８％となっています。日本では、北朝鮮を脅威だと感じる人ほど「話し合い」による解決が有効だと考える人が増えるのに対し、アメリカでは逆に「話し合い」が減る傾向にあります。\\n\\n\\n\\n\\nまた、トランプ大統領は核戦力の強化に意欲を示していますが、核兵器の是非について聞いたところ、アメリカでは「核兵器はすべてなくすべきだ」が５４％、「核兵器は戦争を抑止するために必要だ」が４２％で、「なくすべき」が「必要」を上回りました。一方で、日本では「核兵器はすべてなくすべきだ」が７２％、「核兵器は戦争を抑止するために必要だ」が２０％で、性別や年代、支持政党などにかかわらず、どの層でも核兵器をなくすべきだという人が必要だという人を大きく上回りました。\\n\\n\\n\\nこのほか日米同盟についても聞きました。互いの国について、信頼し、協力しあえる同盟国だと思うかについて、日本では「そう思う」が４９％、「そう思わない」が３５％、アメリカでは「そう思う」が５７％、「そう思わない」が２８％で、日米ともに「信頼できる」という回答が、「そう思わない」を大きく上回りました。\\n\\n\\n\\n一方で、日本にとってアメリカと中国のどちらの国がより重要か、アメリカにとって日本と中国のどちらの国がより重要か、それぞれ聞いたところ、日本では、「アメリカ」が６６％、「どちらも同じくらい重要」が１１％、「中国」が１０％で、アメリカをあげる人が最も多かったのに対し、アメリカでは、「中国」が５８％、「日本」が２９％、「どちらも同じくらい重要」が５％で、中国をあげる人が最も多くなり、日米の意識のずれが鮮明になっています。',\n 'genre': ['北朝鮮情勢', '国際'],\n 'keywords': [],\n 'url': 'https://web.archive.org/web/20180104074901/http://www3.nhk.or.jp/news/html/20180104/k10011278991000.html',\n 'datePublished': '2018-01-04T16:34',\n 'dateModified': ''}"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "archiveurl = 'https://web.archive.org/web/20180104074901/http://www3.nhk.or.jp/news/html/20180104/k10011278991000.html'\n",
    "driver.get(archiveurl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'id': 'k10011276871000',\n 'title': 'トランプ政権 北朝鮮対応「外交の時間限られている」',\n 'article': 'アメリカのトランプ政権は、北朝鮮が早ければことしアメリカ本土に届く核ミサイルを実戦配備する可能性があるとして、中国やロシアをはじめ各国に外交による解決のための時間は限られていると訴えて、さらなる圧力を迫る構えです。\\n北朝鮮への対応をめぐりアメリカのトランプ政権は、制裁などで圧力を最大限まで高めて北朝鮮の行動を改めさせ、事態の打開を目指しています。北朝鮮が早ければことし、アメリカ本土に届く核ミサイルを実戦配備する可能性があるとする情報機関の分析も伝えられ、ホワイトハウスの安全保障担当のマクマスター大統領補佐官も先月、「時間は非常に少なくなっている」と話しています。このためトランプ政権としては、先月、採択された国連の安全保障理事会の新たな制裁決議の履行の徹底とともに、今月、カナダで関係国による外相会合を開き、一層圧力を強化する方策を協議する予定です。そのうえで北朝鮮が行動を改めれば対話に入ることも可能だとするとともに、韓国から提案のあった来月のオリンピック期間中の訓練の延期についても検討を進めると見られます。トランプ政権は、北朝鮮に核ミサイルを保有させないためには軍事的な選択肢も排除しない姿勢を示していて、制裁の効果と北朝鮮の出方を見極めるとともに、中国やロシアをはじめ各国に外交による解決のための時間は限られていると訴えてさらなる圧力を迫る構えです。\\n\\n\\nアメリカのＣＩＡ＝中央情報局で北朝鮮情勢の分析を担当したシンクタンク、ＣＳＩＳ＝戦略国際問題研究所のテリー上席研究員は、トランプ政権の北朝鮮への対応について、「キム・ジョンウン（金正恩）政権に最大限の圧力をかけるアプローチは正しい」として、政策の方向性は正しいと評価しています。ただ、「トランプ大統領は、いくつかの戦術的なミスも犯した」とも述べ、北朝鮮の完全な壊滅に言及するなど、過激な表現を用いたことが、必要以上に緊張を高めることにつながったとして、メッセージの発信には慎重さも求められると指摘します。そして、北朝鮮情勢をめぐることしの注目点として、韓国でオリンピックとパラリンピックが予定される２月から３月を挙げ、「韓国は同時期に例年行われるアメリカとの合同軍事演習の延期を検討している。演習が延期され、北朝鮮が弾道ミサイルの発射や核実験を停止すれば、対話の窓も、少しは開くかもしれない」と述べ、アメリカや北朝鮮の出方を注視しているといいます。さらに、北朝鮮の核・ミサイル開発が、トランプ政権にとって容認できない段階に近づいているとして、「北朝鮮の核開発は完成まで９５％のレベルだ。いまやめると言うなら、アメリカも対応を検討できる。アメリカの行動は、北朝鮮の出方次第だ」と述べ、北朝鮮が非核化の姿勢を見せ、事態が打開に向かうか、分岐点にさしかかるとの見方を示しています。',\n 'genre': ['北朝鮮情勢', 'ピョンチャン五輪', 'トランプ大統領', '国際', '政治'],\n 'keywords': [],\n 'url': 'https://web.archive.org/web/20180101052653/http://www3.nhk.or.jp/news/html/20180101/k10011276871000.html',\n 'datePublished': '2018-01-01T14:22',\n 'dateModified': ''}"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "html = driver.page_source.encode('utf-8')\n",
    "scrape_one_old(html, archiveurl)"
   ]
  }
 ]
}