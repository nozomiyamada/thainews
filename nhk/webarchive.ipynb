{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bit72de44cd76184052b9457c2863c13ac2",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, json, csv, requests, time, glob, tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    json_data = json.loads(soup.find_all(\"script\", type=\"application/ld+json\")[-1].text)\n",
    "    title = json_data.get('headline', soup.find('h1', class_='content--title').text)\n",
    "    date = json_data.get('datePublished', re.search(r'datetime:.*?(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2})', str(html)).group(1))\n",
    "    date_m = json_data.get('dateModified', '')\n",
    "    genre = json_data.get('genre', [])\n",
    "    if genre == []:\n",
    "        genre = [k for k in soup.find('meta', attrs={'name':'keywords'}).get('content').split(',') if k not in ['NHK','ニュース', 'NHK NEWS WEB']]\n",
    "    keywords = json_data.get('keywords', [])\n",
    "    article = soup.find('div', id=\"news_textbody\").text\n",
    "    url_normal = 'https:' + soup.find('meta', property=\"og:url\").get('content').rsplit('https:')[-1]\n",
    "    if soup.find_all('div', id=\"news_textmore\") != []:\n",
    "        for textmore in soup.find_all('div', id=\"news_textmore\"):\n",
    "            article += ('\\n' + textmore.text)\n",
    "    if soup.find_all('div', class_=\"news_add\") != []:\n",
    "        for newsadd in soup.find_all('div', class_=\"news_add\"):\n",
    "            if newsadd.h3 != None:\n",
    "                newsadd.h3.extract()\n",
    "            article += ('\\n' + newsadd.text)\n",
    "            \n",
    "    return {\n",
    "        'id':url_normal.split('/')[-1].split('.html')[0],\n",
    "        'title':title,\n",
    "        'article':article.strip(),\n",
    "        'genre':genre,\n",
    "        'keywords':keywords,\n",
    "        'url':url_normal,\n",
    "        'datePublished':date,\n",
    "        'dateModified':date_m\n",
    "    }\n",
    "\n",
    "def remove_rt(text):\n",
    "    return re.sub('<rt>.+?</rt>', '', text)\n",
    "\n",
    "def tag(text):\n",
    "    text = re.sub(r'<span class=\"colorC\">(.+?)</span>', r\"{org}\\1{/org}\", text)\n",
    "    text = re.sub(r'<span class=\"colorL\">(.+?)</span>', r\"{plc}\\1{/plc}\", text)\n",
    "    text = re.sub(r'<span class=\"colorN\">(.+?)</span>', r\"{per}\\1{/per}\", text)\n",
    "    return text\n",
    "\n",
    "def retag(text):\n",
    "    text = re.sub(r'{org}(.+?){/org}', r\"<org>\\1</org>\", text)\n",
    "    text = re.sub(r'{plc}(.+?){/plc}', r\"<plc>\\1</plc>\", text)\n",
    "    text = re.sub(r'{per}(.+?){/per}', r\"<per>\\1</per>\", text)\n",
    "    return text\n",
    "\n",
    "def remove_a(text):\n",
    "    text = re.sub(r'</?a.*?>', '', text)\n",
    "    text = re.sub(r'<span class=\"under\">(\\w+)</span>', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "# for old web easy\n",
    "def scrape_easy_one(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    url_normal = soup.find('div', id=\"regularnews\").a.get('href').split('/http://')[-1]\n",
    "    url_normal = 'http://' + url_normal\n",
    "    date = soup.find('p', id=\"newsDate\").text[1:-1]\n",
    "    #title_easy = soup.find('h1', class_=\"article-main__title\")\n",
    "    #title_easy_ruby = ''.join([str(t) for t in title_easy.contents]).strip()\n",
    "    url_easy = soup.find('meta', attrs={'name':'shorturl'}).get('content')\n",
    "    title_easy = soup.find('div', id='newstitle').h2\n",
    "    title_easy_ruby = ''.join([str(t) for t in title_easy.contents]).strip()\n",
    "    title_easy = BeautifulSoup(remove_rt(str(title_easy)), \"html.parser\").text.strip()\n",
    "    article_easy = soup.find('div', id=\"newsarticle\")\n",
    "    article_easy = BeautifulSoup(tag(remove_rt(str(article_easy))), \"html.parser\").text.strip()\n",
    "    article_easy_ruby = soup.find('div', id=\"newsarticle\").find_all('p')\n",
    "    article_easy_ruby = '\\n'.join([''.join([remove_a(str(l)) for l in p.contents]) for p in article_easy_ruby if p != []]).strip()\n",
    "    \n",
    "    return {\n",
    "        'id':url_easy.split('/')[-1].split('.html')[0],\n",
    "        'title_easy':title_easy,\n",
    "        'title_easy_ruby':title_easy_ruby,\n",
    "        'article_easy':retag(article_easy),\n",
    "        'article_easy_ruby':article_easy_ruby,\n",
    "        'url_easy':url_easy,\n",
    "        'url_normal':url_normal,\n",
    "        'date_easy':date\n",
    "    }\n",
    "\n",
    "def get_link(start=0):\n",
    "    notyet = []\n",
    "    n_list = pd.read_json('nhkweb.json', encoding='utf-8')['url'].tolist()\n",
    "    df_e = pd.read_json('nhkwebeasy.json', encoding='utf-8') \n",
    "    for i in df_e['url_normal'][start:]:\n",
    "        if i not in n_list:\n",
    "            notyet.append(i)\n",
    "    return notyet\n",
    "\n",
    "def get_link_nogenre(start=0):\n",
    "    notyet = []\n",
    "    with open('nhkweb.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    for dic in data:\n",
    "        if dic['genre'] == []:\n",
    "            notyet.append(dic['url'])\n",
    "    return notyet  \n",
    "\n",
    "def js(dic):\n",
    "    with open('nhkweb.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    with open('nhkweb.json', 'w', encoding='utf-8') as f:\n",
    "        if dic['id'] not in [x['id'] for x in data]:\n",
    "            data.append(dic)\n",
    "        else:\n",
    "            for i, d in enumerate(data):\n",
    "                if dic['id'] == d['id']:\n",
    "                    data[i] = dic\n",
    "        data = sorted(data, key=lambda x:x['id'])\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "def js_e(dic):\n",
    "    with open('nhkwebeasy.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    with open('nhkwebeasy.json', 'w', encoding='utf-8') as f:\n",
    "        if dic['id'] not in [x['id'] for x in data]:\n",
    "            data.append(dic)\n",
    "        else:\n",
    "            for i, d in enumerate(data):\n",
    "                if dic['id'] == d['id']:\n",
    "                    data[i] = dic\n",
    "        data = sorted(data, key=lambda x:x['id'])\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmls = ['https://www3.nhk.or.jp/news/html/20191117/k10012180511000.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['https://www3.nhk.or.jp/news/html/20190120/k10011784611000.html',\n 'https://www3.nhk.or.jp/news/html/20191010/k10012120321000.html',\n 'https://www3.nhk.or.jp/news/html/20191017/k10012135411000.html',\n 'https://www3.nhk.or.jp/news/html/20200204/k10012271251000.html',\n 'https://www3.nhk.or.jp/news/html/20200204/k10012271311000.html']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmls = get_link(0); htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmls = get_link_nogenre(); htmls[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": " 14%|█▍        | 1/7 [01:22<08:16, 82.81s/it]"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-572e0ce6e3f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://web.archive.org'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msnap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for nhkurl in tqdm.tqdm(htmls[:]):\n",
    "    driver.get(f'https://web.archive.org/web/2019*/{nhkurl}')\n",
    "    time.sleep(10)\n",
    "    html = str(driver.page_source.encode('utf-8'))\n",
    "    snap = re.search(r'(between|1 time).*?<a href=\"(.+?)\">', html)\n",
    "    if not snap:\n",
    "        print(nhkurl)\n",
    "        continue\n",
    "    driver.get('https://web.archive.org' + snap.group(2))\n",
    "    time.sleep(30)\n",
    "    html = driver.page_source.encode('utf-8')\n",
    "    dic = scrape(html)\n",
    "    js(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'id': 'k10011846961000', 'title': '買い物弱者など解決へ ロボットがコンビニの商品配達する実験', 'article': '高齢化に伴う「買い物弱者」などの解決につなげようと、自動で動くロボットがコンビニの商品を配達する実験が神奈川県の大学で行われました。\\nこれは大手コンビニチェーンの「ローソン」と自動運転技術を開発するベンチャー企業の「ＺＭＰ」などが行いました。実験は大学内で行われ、利用者がスマートフォンの専用のアプリを使って商品を注文すると、高さ、幅などが１メートルに満たない箱型のロボットが、仮設の店舗から商品を届けます。センサーやカメラが周りの状況を認識しているということで、人が近づくと自動的に停止するほか、人とすれ違った際には音声であいさつもしてくれます。両社では、こうしたロボットで高齢化に伴う買い物弱者やドライバーの人手不足といった課題の解決につなげたい考えです。また公道を移動できるよう国に対して法整備を働きかけていくことにしています。ローソンの牧野国嗣オープン・イノベーションセンター長は「時間がなかったり、体が不自由で店に行けなかったりするなど、さまざまなニーズに応えるため、実用化を進めていきたい」と話していました。', 'genre': ['ビジネス', '社会'], 'keywords': [], 'url': 'https://www3.nhk.or.jp/news/html/20190313/k10011846961000.html', 'datePublished': '2019-03-13T17:35:18+09:00', 'dateModified': '2019-03-27T14:00:23+09:00'}\n"
    }
   ],
   "source": [
    "html = driver.page_source.encode('utf-8')\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "dic = scrape(html)\n",
    "print(dic)\n",
    "js(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = str(driver.page_source.encode('utf-8'))\n",
    "re.search(r'\"genre\":(.+?),', html)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean category & keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "5147\n1685\n"
    }
   ],
   "source": [
    "with open('nhkweb.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    print(len(data))\n",
    "with open('nhkwebeasy.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "articles:  5147\n"
    },
    {
     "data": {
      "text/plain": "[('社会', 1639),\n ('国際', 1322),\n ('ビジネス', 823),\n ('科学・文化', 729),\n ('スポーツ', 702),\n ('政治', 615),\n ('暮らし', 410),\n ('地域', 319),\n ('気象・災害', 271)]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check category\n",
    "\n",
    "with open('nhkweb.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print('articles: ', len(data))\n",
    "genre = Counter()\n",
    "for dic in data:\n",
    "    for g in dic['genre']:\n",
    "        genre[g] += 1\n",
    "genre.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genre <> keywords\n",
    "\n",
    "with open('nhkweb.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "category = ['社会', '国際', 'ビジネス', 'スポーツ', '政治', '科学・文化', '暮らし', '地域', '気象・災害']\n",
    "for i, dic in enumerate(data):\n",
    "    newgenre = []\n",
    "    newkey = []\n",
    "    for j in dic['genre']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    for j in dic['keywords']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    data[i]['genre'] = list(set(newgenre))\n",
    "    data[i]['keywords'] = list(set(newkey))\n",
    "\n",
    "with open('nhkweb.json','w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "k10012255531000    1\nk10012263311000    1\nk10012263971000    1\nk10012228721000    1\nk10012251101000    1\n                  ..\nk10011795521000    1\nk10012234731000    1\nk10012253721000    1\nk10012084571000    1\nk10012239591000    1\nName: id, Length: 5147, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal = pd.read_json('nhkweb.json')\n",
    "normal.id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web easy archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "858\n"
    },
    {
     "data": {
      "text/plain": "['http://www3.nhk.or.jp/news/easy/k10010344231000/k10010344231000.html',\n 'http://www3.nhk.or.jp/news/easy/k10010344561000/k10010344561000.html',\n 'http://www3.nhk.or.jp/news/easy/k10010347151000/k10010347151000.html',\n 'http://www3.nhk.or.jp/news/easy/k10010347301000/k10010347301000.html',\n 'http://www3.nhk.or.jp/news/easy/k10010347461000/k10010347461000.html',\n 'http://www3.nhk.or.jp/news/easy/k10010347961000/k10010347961000.html',\n 'http://www3.nhk.or.jp/news/easy/k10010348521000/k10010348521000.html',\n 'http://www3.nhk.or.jp/news/easy/k10010348991000/k10010348991000.html',\n 'http://www3.nhk.or.jp/news/easy/k10010349161000/k10010349161000.html',\n 'http://www3.nhk.or.jp/news/easy/k10010349221000/k10010349221000.html']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tobescraped.txt') as f:\n",
    "    lst = f.read().split()\n",
    "with open('nolink.txt') as f:\n",
    "    nolink = f.read().split()\n",
    "with open('nhkwebeasy.json') as f:\n",
    "    urls = [x['url_easy'] for x in json.load(f)]\n",
    "htmls = sorted(set(lst) - set(urls) - set(nolink))\n",
    "print(len(htmls))\n",
    "htmls[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "1%|          | 3/480 [02:39<6:56:18, 52.37s/it]http://www3.nhk.or.jp/news/easy/k10010319131000/k10010319131000.html\n  1%|          | 4/480 [02:50<5:17:36, 40.03s/it]http://www3.nhk.or.jp/news/easy/k10010319391000/k10010319391000.html\n  1%|▏         | 7/480 [04:54<4:57:15, 37.71s/it]http://www3.nhk.or.jp/news/easy/k10010319891000/k10010319891000.html\n  2%|▏         | 8/480 [05:06<3:54:45, 29.84s/it]http://www3.nhk.or.jp/news/easy/k10010320161000/k10010320161000.html\n  2%|▎         | 12/480 [09:16<6:10:15, 47.47s/it]http://www3.nhk.or.jp/news/easy/k10010344561000/k10010344561000.html\n  3%|▎         | 13/480 [09:28<4:45:10, 36.64s/it]http://www3.nhk.or.jp/news/easy/k10010347151000/k10010347151000.html\n  4%|▍         | 20/480 [15:44<5:37:08, 43.97s/it]http://www3.nhk.or.jp/news/easy/k10010349221000/k10010349221000.html\n 27%|██▋       | 128/480 [2:07:38<5:21:02, 54.72s/it]http://www3.nhk.or.jp/news/easy/k10010409121000/k10010409121000.html\n"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e5eae2709dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_easy_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mjs_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-971cc75274e9>\u001b[0m in \u001b[0;36mscrape_easy_one\u001b[0;34m(html)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_easy_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0murl_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"regularnews\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/http://'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0murl_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"newsDate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "for nhkurl in tqdm.tqdm(htmls[20:500]):\n",
    "    driver.get(f'https://web.archive.org/web/2016*/{nhkurl}')\n",
    "    time.sleep(10)\n",
    "    html = str(driver.page_source.encode('utf-8'))\n",
    "    snap = re.search(r'(between|1 time).*?<a href=\"(.+?)\">', html)\n",
    "    if not snap:\n",
    "        print(nhkurl)\n",
    "        continue\n",
    "    driver.get('https://web.archive.org' + snap.group(2))\n",
    "    time.sleep(20)\n",
    "    html = driver.page_source.encode('utf-8')\n",
    "    dic = scrape_easy_one(html)\n",
    "    js_e(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "13%|█▎        | 14/110 [12:00<57:19, 35.83s/it]http://www3.nhk.or.jp/news/easy/k10010326291000/k10010326291000.html\n 34%|███▎      | 37/110 [33:11<50:41, 41.66s/it]http://www3.nhk.or.jp/news/easy/k10010335581000/k10010335581000.html\n 45%|████▍     | 49/110 [45:01<47:24, 46.63s/it]http://www3.nhk.or.jp/news/easy/k10010340871000/k10010340871000.html\n 54%|█████▎    | 59/110 [56:01<1:04:36, 76.01s/it]"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-898e19c96a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_easy_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mjs_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-971cc75274e9>\u001b[0m in \u001b[0;36mscrape_easy_one\u001b[0;34m(html)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_easy_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0murl_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"regularnews\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/http://'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0murl_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"newsDate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "for nhkurl in tqdm.tqdm(htmls[40:150]):\n",
    "    driver.get(f'https://web.archive.org/web/2016*/{nhkurl}')\n",
    "    time.sleep(10)\n",
    "    html = str(driver.page_source.encode('utf-8'))\n",
    "    snap = re.search(r'(between|1 time).*?<a href=\"(.+?)\">', html)\n",
    "    if not snap:\n",
    "        print(nhkurl)\n",
    "        continue\n",
    "    driver.get('https://web.archive.org' + snap.group(2))\n",
    "    time.sleep(20)\n",
    "    html = driver.page_source.encode('utf-8')\n",
    "    dic = scrape_easy_one(html)\n",
    "    js_e(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source.encode('utf-8')\n",
    "dic = scrape_easy_one(html)\n",
    "js_e(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}