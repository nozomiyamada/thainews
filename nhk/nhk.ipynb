{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# import & get article ID"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"oldest ID: 185875\nnewest ID: 198988\n546\n"}],"source":["import requests, json, os, csv, re \n","from bs4 import BeautifulSoup as BS\n","\n","with open ('nhk.json', 'r') as f:\n","    ids = sorted(x['id'] for x in json.load(f))\n","    print('oldest ID: %s' % ids[0])\n","    print('newest ID: %s' % ids[-1])\n","    print(len(ids))"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# Scrape"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["total_list = []\n","for i in range(198738, 199200):\n","    url = f\"https://www3.nhk.or.jp/nhkworld/th/news/{i}/\"\n","    response = requests.get(url)\n","    response.encoding='utf-8'\n","    if response.status_code == 200:\n","        dic = {}\n","        soup = BS(response.text, \"html.parser\")\n","        data = soup.find('script',type=\"application/ld+json\")\n","        data = json.loads(data.text)\n","        dic['headline'] = data['headline']\n","        dic['article'] = data['articleBody']\n","        dic['date'] = data['datePublished']\n","        dic['url'] = url\n","        dic['id'] = str(i)\n","        total_list.append(dic)\n","\n","with open ('nhk_new.json', 'w') as f:\n","    with open ('nhk.json', 'r') as g:\n","        old_list = json.load(g)\n","        for dic in total_list:\n","            if dic not in old_list:\n","                old_list.append(dic)\n","    json.dump(old_list, f, ensure_ascii=False, indent=4)"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# Delete & Rename"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["if os.path.exists('nhk.json') and os.path.exists('nhk_new.json') and os.path.getsize('nhk_new.json') > os.path.getsize('nhk.json'):\n","    os.remove('nhk.json')\n","    os.rename('nhk_new.json', 'nhk.json')"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# tokenize"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from pythainlp import word_tokenize as wt\n","def clean(text):\n","    text = re.sub(r'https?://\\S* ', '', text)\n","    text = re.sub(r'\\r?(\\n|\\t)', ' ', text)\n","    text = re.sub(r'\"+', '', text)\n","    return wt(text, keep_whitespace=False)\n","\n","with open('nhk.json') as f:\n","    data = json.load(f)\n","with open('nhk_tokenized.txt','w') as f:\n","    tokenized = [clean(news_dic['article']) for news_dic in data]\n","    writer = csv.writer(f, lineterminator='\\n', delimiter=' ')\n","    writer.writerows(tokenized)"]},{"cell_type":"markdown","execution_count":29,"metadata":{},"outputs":[],"source":["# Word2Vec\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from gensim.models import word2vec\n","model = word2vec.Word2Vec(tokenized, size=200, min_count=3, window=5, iter=100)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":"3143"},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["len(model.wv.vocab)"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# word frequency"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import collections \n","from pythainlp import corpus\n","count = collections.Counter()\n","stop = collections.Counter()\n","stops = corpus.thai_stopwords()\n","for line in tokenized:\n","    for word in line:\n","        count[word] += 1\n","        if word not in stops and word not in ['2', '-']:\n","            stop[word] += 1"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"ที่ ของ ใน และ ได้ ว่า การ ญี่ปุ่น มี จะ นาย เป็น นี้ วัน ให้ เมื่อ สหรัฐ กับ จาก ซึ่ง โดย เพื่อ ไม่ ระบุ คน ตุลาคม ขึ้น ยัง ปี ไป"}],"source":["for i in count.most_common(30):\n","    print(i[0], end=' ')"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"ญี่ปุ่น สหรัฐ ระบุ คน ตุลาคม ปี จีน กล่าวว่า เกาหลีใต้ ที่จะ จังหวัด ประธานาธิบดี เดือน รัฐบาล เจ้าหน้าที่ ชิ พฤศจิกายน ต่าง ๆ เกาหลีเหนือ เรื่อง ประเทศ เมือง ผู้นำ ฮา บริษัท โตเกียว กรุง รัฐมนตรี เรียกร้อง ฮ่องกง"}],"source":["for i in stop.most_common(30):\n","    print(i[0], end=' ')"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":"[('พารา', 0.5588167905807495),\n ('เหรียญทอง', 0.5159199833869934),\n ('สตรี', 0.4935027062892914),\n ('โง', 0.46441853046417236),\n ('ไซ', 0.4540303945541382),\n ('เชื่อม', 0.45229601860046387),\n ('เจ้าของ', 0.44493910670280457),\n ('022', 0.43452709913253784),\n ('ฮาเนดะ', 0.4340747594833374),\n ('ประทับ', 0.4332934319972992)]"},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["model.wv.most_similar('โอซากา')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np \n","A = np.array([[1,2]])"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}