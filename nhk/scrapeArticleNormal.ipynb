{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bit72de44cd76184052b9457c2863c13ac2",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, json, csv, requests, time, glob, tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_one_new(html, url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    json_data = json.loads(soup.find_all(\"script\", type=\"application/ld+json\")[-1].text)\n",
    "\n",
    "    # title, date, genre, keyword\n",
    "    title = json_data.get('headline', soup.find('span', class_='contentTitle').text)\n",
    "    date = json_data.get('datePublished', re.search(r'datetime:.*?(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2})', str(html)).group(1))\n",
    "    date_m = json_data.get('dateModified', '')\n",
    "    genre = json_data.get('genre', [])\n",
    "    if genre == []:\n",
    "        genre = [k for k in soup.find('meta', attrs={'name':'keywords'}).get('content').split(',') if k not in ['NHK','ニュース', 'NHK NEWS WEB']]\n",
    "    keywords = json_data.get('keywords', [])\n",
    "    \n",
    "    # article: news_textbody > news_textmore > news_add (paragraph titles are h3)\n",
    "    article = soup.find('div', id=\"news_textbody\").text\n",
    "    if soup.find_all('div', id=\"news_textmore\") != []:\n",
    "        for textmore in soup.find_all('div', id=\"news_textmore\"):\n",
    "            article += ('\\n' + textmore.text)\n",
    "    if soup.find_all('div', class_=\"news_add\") != []:\n",
    "        for newsadd in soup.find_all('div', class_=\"news_add\"):\n",
    "            if newsadd.h3 != None:\n",
    "                newsadd.h3.extract()\n",
    "            article += ('\\n' + newsadd.text)\n",
    "            \n",
    "    return {\n",
    "        'id':url.split('/')[-1].split('.html')[0],\n",
    "        'title':title.strip(),\n",
    "        'article':article.strip(),\n",
    "        'genre':genre,\n",
    "        'keywords':keywords,\n",
    "        'url':url,\n",
    "        'datePublished':date,\n",
    "        'dateModified':date_m\n",
    "    }\n",
    "\n",
    "# for old web normal\n",
    "def make_datetime_normal_old(hmd, time):\n",
    "    year, month, day = hmd[:4], hmd[4:6], hmd[6:]\n",
    "    hour, minute = time.split('時')\n",
    "    minute = minute.strip('分')\n",
    "    if len(hour) == 1:\n",
    "        hour = '0' + hour\n",
    "    if len(minute) == 1:\n",
    "        minute = '0' + minute\n",
    "    return f\"{year}-{month}-{day}T{hour}:{minute}\"\n",
    "\n",
    "def scrape_one_old(html, url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # title, date, genre, keyword\n",
    "    title = soup.find('span', class_=\"contentTitle\").text.strip()\n",
    "    hmd_ = url.split('/')[-2]\n",
    "    time_ = soup.find('span', id=\"news_time\").text\n",
    "    date = make_datetime_normal_old(hmd_, time_)\n",
    "    genre = [k for k in soup.find('meta', attrs={'name':'keywords'}).get('content').split(',') if k not in ['NHK','ニュース', 'NHK NEWS WEB','ＮＨＫ','ＮＨＫニュース','']]\n",
    "    \n",
    "    # article: news_textbody > news_textmore > news_add (paragraph titles are h3)\n",
    "    article = soup.find(['div','p'], id=\"news_textbody\").text\n",
    "    if soup.find_all(['div','p'], id=\"news_textmore\") != []:\n",
    "        for textmore in soup.find_all(['div','p'], id=\"news_textmore\"):\n",
    "            article += ('\\n' + textmore.text)\n",
    "    if soup.find_all(['div','p'], class_=\"news_add\") != []:\n",
    "        for newsadd in soup.find_all(['div','p'], class_=\"news_add\"):\n",
    "            if newsadd.h3 != None:\n",
    "                newsadd.h3.extract()\n",
    "            article += ('\\n' + newsadd.text)\n",
    "            \n",
    "    return {\n",
    "        'id':url.split('/')[-1].split('.html')[0],\n",
    "        'title':title.strip(),\n",
    "        'article':article.strip(),\n",
    "        'genre':genre,\n",
    "        'keywords':[],\n",
    "        'url':url,\n",
    "        'datePublished':date,\n",
    "        'dateModified':\"\"\n",
    "    }\n",
    "\n",
    "def get_archiveurl(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    html = str(driver.page_source.encode('utf-8'))\n",
    "    snap = re.search(r'(times between|1 time|times).*?<a href=\"(.+?)\">', html)\n",
    "    #if snap == None:\n",
    "        #return None\n",
    "    archiveurl = 'https://web.archive.org' + snap.group(2)\n",
    "    return archiveurl\n",
    "\n",
    "def js(dic, year):\n",
    "    with open(f'nhkweb{year}.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    with open(f'nhkweb{year}.json', 'w', encoding='utf-8') as f:\n",
    "        if dic['id'] not in [x['id'] for x in data]:\n",
    "            data.append(dic)\n",
    "        else:\n",
    "            for i, d in enumerate(data):\n",
    "                if dic['id'] == d['id']:\n",
    "                    data[i] = dic\n",
    "        data = sorted(data, key=lambda x:x['id'])\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def geturl(year=2019):\n",
    "    idnormal = pd.read_json(f'nhkweb{year}.json', encoding='utf-8')['id'].tolist()\n",
    "    existurl = pd.read_csv('linknormal.txt', encoding='utf-8', header=None)[0].tolist()\n",
    "    nolink = pd.read_csv('nolinknormal.txt', encoding='utf-8', header=None)[0].tolist()\n",
    "    urls = set(existurl) - set(nolink)\n",
    "    return sorted([url for url in urls if (url.split('.html')[0].split('/')[-1] not in idnormal) and f'html/{year}' in url])\n",
    "\n",
    "def checkwrongid(): # check wrong ID in newswebeasy\n",
    "    df = pd.read_json('nhkwebeasy.json', encoding='utf-8')\n",
    "    print(len(df))\n",
    "    df['normalID'] = df['url_normal'].apply(lambda x:x.split('/')[-1].strip('.html'))\n",
    "    return df[df['id'] != df['normalID']]['id'].tolist()\n",
    "\n",
    "def wrongscrape():\n",
    "    wrongids = wrongid()\n",
    "    existurl = pd.read_csv('linknormal.txt', encoding='utf-8', header=None)[0].tolist()[::-1]\n",
    "    for ID in wrongids:\n",
    "        for url in existurl:\n",
    "            if ID in url:\n",
    "                print(url.split('/*/')[-1])\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('nhkweb2018.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>article</th>\n      <th>genre</th>\n      <th>keywords</th>\n      <th>url</th>\n      <th>datePublished</th>\n      <th>dateModified</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4106</th>\n      <td>k10011423691000</td>\n      <td>チーズや納豆 小麦粉も 今月以降値上げ相次ぐ</td>\n      <td>今月から来月にかけても、チーズや小麦粉など暮らしに身近な商品の値上げが相次ぎます。\\nこのう...</td>\n      <td>[暮らし, ビジネス]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20180501/k1001...</td>\n      <td>2018-05-01T04:25</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4107</th>\n      <td>k10011423711000</td>\n      <td>“東京で映画のロケを” カンヌでＰＲへ</td>\n      <td>海外映画の都内でのロケを誘致しようと、今月開かれる「カンヌ映画祭」の見本市で、映画製作を支援...</td>\n      <td>[科学・文化]</td>\n      <td>[エンタメ]</td>\n      <td>http://www3.nhk.or.jp/news/html/20180501/k1001...</td>\n      <td>2018-05-01T07:52</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4108</th>\n      <td>k10011423741000</td>\n      <td>北九州６人死亡火災 防犯カメラに不審人物 放火の疑いで捜査</td>\n      <td>去年５月、北九州市で、日雇いの労働者などが暮らすアパートが全焼して６人が死亡した火事で、火が...</td>\n      <td>[社会]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20180501/k1001...</td>\n      <td>2018-05-01T05:12</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4109</th>\n      <td>k10011423781000</td>\n      <td>トランプ大統領 米朝首脳会談の開催地にパンムンジョムも検討</td>\n      <td>アメリカのトランプ大統領は、史上初となる北朝鮮との首脳会談の開催地をめぐり、東南アジアのシン...</td>\n      <td>[社会, 国際]</td>\n      <td>[米朝首脳会談, トランプ大統領, 国際１]</td>\n      <td>http://www3.nhk.or.jp/news/html/20180501/k1001...</td>\n      <td>2018-05-01T04:44</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4110</th>\n      <td>k10011423801000</td>\n      <td>住宅１棟が全焼 １人死亡 東京 三鷹</td>\n      <td>３０日夜、東京 三鷹市の住宅街で住宅１棟が全焼する火事があり、１人が遺体で見つかりました。警...</td>\n      <td>[社会]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20180501/k1001...</td>\n      <td>2018-05-01T01:37</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5119</th>\n      <td>k10011459731000</td>\n      <td>“輸入車に高関税”  日本貿易会会長「官民連携で阻止したい」</td>\n      <td>アメリカのトランプ政権が輸入車に高い関税をかける可能性が出ていることについて、日本貿易会の中...</td>\n      <td>[ビジネス, 国際]</td>\n      <td>[トランプ大統領]</td>\n      <td>http://www3.nhk.or.jp/news/html/20180531/k1001...</td>\n      <td>2018-05-31T14:49</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5120</th>\n      <td>k10011459781000</td>\n      <td>“車いすのまま避難” 最新の防災技術・製品紹介の展示会 東京</td>\n      <td>防災に関する最新の技術や製品などを紹介する大規模な展示会が、３１日都内で始まりました。\\nこ...</td>\n      <td>[社会, 気象・災害]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20180531/k1001...</td>\n      <td>2018-05-31T15:08</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5121</th>\n      <td>k10011459801000</td>\n      <td>“医療保険 経済力に応じた負担” 抜本改革案提示を首相に提言</td>\n      <td>６年後には、５０歳以上が人口の５割を超えることを見据え、自民党は医療保険制度の自己負担を、年...</td>\n      <td>[政治]</td>\n      <td>[医療]</td>\n      <td>http://www3.nhk.or.jp/news/html/20180531/k1001...</td>\n      <td>2018-05-31T15:16</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5122</th>\n      <td>k10011459811000</td>\n      <td>新潟 五頭連峰で発見の２遺体 遭難した親子と確認</td>\n      <td>２９日に新潟県阿賀野市の五頭連峰で遺体で見つかった２人は、警察が調べたところ、今月５日に五頭...</td>\n      <td>[社会]</td>\n      <td>[登山の親子不明]</td>\n      <td>http://www3.nhk.or.jp/news/html/20180531/k1001...</td>\n      <td>2018-05-31T15:26</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5123</th>\n      <td>k10011459851000</td>\n      <td>「半分、青い。」切手 知事に贈呈 岐阜</td>\n      <td>岐阜県東部がヒロインの故郷となっているＮＨＫの連続テレビ小説、「半分、青い。」の切手シートが...</td>\n      <td>[暮らし, 地域]</td>\n      <td>[エンタメ]</td>\n      <td>http://www3.nhk.or.jp/news/html/20180531/k1001...</td>\n      <td>2018-05-31T15:54</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n<p>1018 rows × 8 columns</p>\n</div>",
      "text/plain": "                   id                           title  \\\n4106  k10011423691000          チーズや納豆 小麦粉も 今月以降値上げ相次ぐ   \n4107  k10011423711000             “東京で映画のロケを” カンヌでＰＲへ   \n4108  k10011423741000   北九州６人死亡火災 防犯カメラに不審人物 放火の疑いで捜査   \n4109  k10011423781000   トランプ大統領 米朝首脳会談の開催地にパンムンジョムも検討   \n4110  k10011423801000              住宅１棟が全焼 １人死亡 東京 三鷹   \n...               ...                             ...   \n5119  k10011459731000  “輸入車に高関税”  日本貿易会会長「官民連携で阻止したい」   \n5120  k10011459781000  “車いすのまま避難” 最新の防災技術・製品紹介の展示会 東京   \n5121  k10011459801000  “医療保険 経済力に応じた負担” 抜本改革案提示を首相に提言   \n5122  k10011459811000        新潟 五頭連峰で発見の２遺体 遭難した親子と確認   \n5123  k10011459851000             「半分、青い。」切手 知事に贈呈 岐阜   \n\n                                                article        genre  \\\n4106  今月から来月にかけても、チーズや小麦粉など暮らしに身近な商品の値上げが相次ぎます。\\nこのう...  [暮らし, ビジネス]   \n4107  海外映画の都内でのロケを誘致しようと、今月開かれる「カンヌ映画祭」の見本市で、映画製作を支援...      [科学・文化]   \n4108  去年５月、北九州市で、日雇いの労働者などが暮らすアパートが全焼して６人が死亡した火事で、火が...         [社会]   \n4109  アメリカのトランプ大統領は、史上初となる北朝鮮との首脳会談の開催地をめぐり、東南アジアのシン...     [社会, 国際]   \n4110  ３０日夜、東京 三鷹市の住宅街で住宅１棟が全焼する火事があり、１人が遺体で見つかりました。警...         [社会]   \n...                                                 ...          ...   \n5119  アメリカのトランプ政権が輸入車に高い関税をかける可能性が出ていることについて、日本貿易会の中...   [ビジネス, 国際]   \n5120  防災に関する最新の技術や製品などを紹介する大規模な展示会が、３１日都内で始まりました。\\nこ...  [社会, 気象・災害]   \n5121  ６年後には、５０歳以上が人口の５割を超えることを見据え、自民党は医療保険制度の自己負担を、年...         [政治]   \n5122  ２９日に新潟県阿賀野市の五頭連峰で遺体で見つかった２人は、警察が調べたところ、今月５日に五頭...         [社会]   \n5123  岐阜県東部がヒロインの故郷となっているＮＨＫの連続テレビ小説、「半分、青い。」の切手シートが...    [暮らし, 地域]   \n\n                    keywords  \\\n4106                      []   \n4107                  [エンタメ]   \n4108                      []   \n4109  [米朝首脳会談, トランプ大統領, 国際１]   \n4110                      []   \n...                      ...   \n5119               [トランプ大統領]   \n5120                      []   \n5121                    [医療]   \n5122               [登山の親子不明]   \n5123                  [エンタメ]   \n\n                                                    url     datePublished  \\\n4106  http://www3.nhk.or.jp/news/html/20180501/k1001...  2018-05-01T04:25   \n4107  http://www3.nhk.or.jp/news/html/20180501/k1001...  2018-05-01T07:52   \n4108  http://www3.nhk.or.jp/news/html/20180501/k1001...  2018-05-01T05:12   \n4109  http://www3.nhk.or.jp/news/html/20180501/k1001...  2018-05-01T04:44   \n4110  http://www3.nhk.or.jp/news/html/20180501/k1001...  2018-05-01T01:37   \n...                                                 ...               ...   \n5119  http://www3.nhk.or.jp/news/html/20180531/k1001...  2018-05-31T14:49   \n5120  http://www3.nhk.or.jp/news/html/20180531/k1001...  2018-05-31T15:08   \n5121  http://www3.nhk.or.jp/news/html/20180531/k1001...  2018-05-31T15:16   \n5122  http://www3.nhk.or.jp/news/html/20180531/k1001...  2018-05-31T15:26   \n5123  http://www3.nhk.or.jp/news/html/20180531/k1001...  2018-05-31T15:54   \n\n     dateModified  \n4106               \n4107               \n4108               \n4109               \n4110               \n...           ...  \n5119               \n5120               \n5121               \n5122               \n5123               \n\n[1018 rows x 8 columns]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['datePublished'].str.startswith('2018-05')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "#options.headless = True\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: window was already closed\n  (Session info: chrome=80.0.3987.132)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-91a43443591d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# get archive URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0marchiveurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_archiveurl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e43317fa45b8>\u001b[0m in \u001b[0;36mget_archiveurl\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0msnap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(times between|1 time|times).*?<a href=\"(.+?)\">'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m#if snap == None:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mpage_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \"\"\"\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_PAGE_SOURCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: window was already closed\n  (Session info: chrome=80.0.3987.132)\n"
     ]
    }
   ],
   "source": [
    "year = 2016\n",
    "urls = pd.read_csv(f'linknormal.txt', header=None)\n",
    "urls = sorted(urls[urls[0].str.contains(f'html/{year}')][0].tolist())\n",
    "id_exist = set(pd.read_json(f'nhkweb{year}.json')['id'].tolist())\n",
    "\n",
    "for url in urls:\n",
    "    # check URL\n",
    "    ID = url.split('.html')[0].split('/')[-1]\n",
    "    if ID in id_exist:\n",
    "        continue\n",
    "\n",
    "    # get archive URL\n",
    "    archiveurl = get_archiveurl(url)\n",
    "\n",
    "    # request\n",
    "    response = requests.get(archiveurl)\n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "    elif response.status_code == 504:\n",
    "        response = requests.get(archiveurl)\n",
    "        if response.status_code == 504:\n",
    "            continue\n",
    "        html = response.text\n",
    "    time.sleep(2)\n",
    "\n",
    "    # scrape\n",
    "    url_true = 'htt' + url.split('/htt')[-1]\n",
    "\n",
    "    try:\n",
    "        dic = scrape_one_new(html, url_true)\n",
    "    except:\n",
    "        dic = scrape_one_old(html, url_true)\n",
    "    js(dic, year)\n",
    "    id_exist.add(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': 'k10011111971000',\n 'title': '中国軍機紀伊半島沖飛行 「断固として守る」 官房長官',\n 'article': '菅官房長官は閣議のあとの記者会見で、中国の爆撃機６機が２４日、沖縄本島と宮古島の間の上空を通過して紀伊半島沖まで飛行したことについて、領土、領海、領空は断固として守るという決意のもとで警戒監視活動に万全を期す考えを強調しました。\\n防衛省によりますと２４日午前、中国軍のＨ６爆撃機６機が東シナ海から沖縄本島と宮古島の間を抜けて太平洋に出たあと紀伊半島の南の沖合まで飛行し、同じ経路で東シナ海方面に戻ったということです。これについて菅官房長官は閣議のあとの記者会見で「領空侵犯はなかったが、このようなルートで中国機の飛行が確認されたのは今回初めてだという報告を受けている」と述べました。そのうえで菅官房長官は「政府としては今後も、活動を拡大し活発化させている中国の動向を注視しながら、わが国の領土、領海、領空は断固として守るという決意のもとに警戒監視活動に万全を期し、国際法および自衛隊法に基づいて厳正な措置を実施していきたい」と述べました。',\n 'genre': ['政治'],\n 'keywords': [],\n 'url': 'http://www3.nhk.or.jp/news/html/20170825/k10011111971000.html',\n 'datePublished': '2017-08-25T12:43',\n 'dateModified': ''}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_true = 'htt' + driver.current_url.split('/htt')[-1]\n",
    "html = driver.page_source.encode('utf-8')\n",
    "try:\n",
    "    dic = scrape_one_new(html, url_true)\n",
    "except:\n",
    "    dic = scrape_one_old(html, url_true)\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'https://web.archive.orgmailto:info@archive.org'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_archiveurl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "articles:  12166\n"
    },
    {
     "data": {
      "text/plain": "[('国際', 3430),\n ('社会', 2933),\n ('スポーツ', 2321),\n ('気象・災害', 1624),\n ('ビジネス', 1445),\n ('政治', 1275),\n ('科学・文化', 688),\n ('地域', 526),\n ('暮らし', 427)]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check category\n",
    "\n",
    "year = 2018\n",
    "\n",
    "with open(f'nhkweb{year}.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print('articles: ', len(data))\n",
    "genre = Counter()\n",
    "for dic in data:\n",
    "    for g in dic['genre']:\n",
    "        genre[g] += 1\n",
    "genre.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genre <> keywords\n",
    "with open(f'nhkweb{year}.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "category = ['社会', '国際', 'ビジネス', 'スポーツ', '政治', '科学・文化', '暮らし', '地域', '気象・災害']\n",
    "for i, dic in enumerate(data):\n",
    "    newgenre = []\n",
    "    newkey = []\n",
    "    for j in dic['genre']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        elif j == \"科学・医療\" or j == \"文化・エンタメ\" or j == \"科学\":\n",
    "            newgenre.append('科学・文化')\n",
    "        elif j == \"暮らし文化\":\n",
    "            newgenre.append('暮らし')\n",
    "            newgenre.append('科学・文化')\n",
    "        elif j == \"経済\":\n",
    "            newgenre.append('ビジネス')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    for j in dic['keywords']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"科学・医療\" or j == \"文化・エンタメ\" or j == \"科学\":\n",
    "            newgenre.append('科学・文化')\n",
    "        elif j == \"暮らし文化\":\n",
    "            newgenre.append('暮らし')\n",
    "            newgenre.append('科学・文化')\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    data[i]['genre'] = list(set(newgenre))\n",
    "    data[i]['keywords'] = list(set(newkey))\n",
    "\n",
    "with open(f'nhkweb{year}.json','w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "k10010967691000    1\nk10010912981000    1\nk10011097851000    1\nk10010989411000    1\nk10011029051000    1\n                  ..\nk10011105661000    1\nk10011010431000    1\nk10010890631000    1\nk10011074431000    1\nk10011002731000    1\nName: id, Length: 7140, dtype: int64"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(f'nhkweb{year}.json').id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}