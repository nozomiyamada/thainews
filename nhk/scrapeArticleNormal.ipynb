{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bit72de44cd76184052b9457c2863c13ac2",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, json, csv, requests, time, glob, tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_one_new(html, url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    json_data = json.loads(soup.find_all(\"script\", type=\"application/ld+json\")[-1].text)\n",
    "\n",
    "    # title, date, genre, keyword\n",
    "    title = json_data.get('headline', soup.find('span', class_='contentTitle').text)\n",
    "    date = json_data.get('datePublished', re.search(r'datetime:.*?(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2})', str(html)).group(1))\n",
    "    date_m = json_data.get('dateModified', '')\n",
    "    genre = json_data.get('genre', [])\n",
    "    if genre == []:\n",
    "        genre = [k for k in soup.find('meta', attrs={'name':'keywords'}).get('content').split(',') if k not in ['NHK','ニュース', 'NHK NEWS WEB']]\n",
    "    keywords = json_data.get('keywords', [])\n",
    "    \n",
    "    # article: news_textbody > news_textmore > news_add (paragraph titles are h3)\n",
    "    article = soup.find('div', id=\"news_textbody\").text\n",
    "    if soup.find_all('div', id=\"news_textmore\") != []:\n",
    "        for textmore in soup.find_all('div', id=\"news_textmore\"):\n",
    "            article += ('\\n' + textmore.text)\n",
    "    if soup.find_all('div', class_=\"news_add\") != []:\n",
    "        for newsadd in soup.find_all('div', class_=\"news_add\"):\n",
    "            if newsadd.h3 != None:\n",
    "                newsadd.h3.extract()\n",
    "            article += ('\\n' + newsadd.text)\n",
    "            \n",
    "    return {\n",
    "        'id':url.split('/')[-1].split('.html')[0],\n",
    "        'title':title.strip(),\n",
    "        'article':article.strip(),\n",
    "        'genre':genre,\n",
    "        'keywords':keywords,\n",
    "        'url':url,\n",
    "        'datePublished':date,\n",
    "        'dateModified':date_m\n",
    "    }\n",
    "\n",
    "# for old web normal\n",
    "def make_datetime_normal_old(hmd, time):\n",
    "    year, month, day = hmd[:4], hmd[4:6], hmd[6:]\n",
    "    hour, minute = time.split('時')\n",
    "    minute = minute.strip('分')\n",
    "    if len(hour) == 1:\n",
    "        hour = '0' + hour\n",
    "    if len(minute) == 1:\n",
    "        minute = '0' + minute\n",
    "    return f\"{year}-{month}-{day}T{hour}:{minute}\"\n",
    "\n",
    "def scrape_one_old(html, url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # title, date, genre, keyword\n",
    "    title = soup.find('span', class_=\"contentTitle\").text.strip()\n",
    "    hmd_ = url.split('/')[-2]\n",
    "    time_ = soup.find('span', id=\"news_time\").text\n",
    "    date = make_datetime_normal_old(hmd_, time_)\n",
    "    genre = [k for k in soup.find('meta', attrs={'name':'keywords'}).get('content').split(',') if k not in ['NHK','ニュース', 'NHK NEWS WEB','ＮＨＫ','ＮＨＫニュース','']]\n",
    "    \n",
    "    # article: news_textbody > news_textmore > news_add (paragraph titles are h3)\n",
    "    article = soup.find(['div','p'], id=\"news_textbody\").text\n",
    "    if soup.find_all(['div','p'], id=\"news_textmore\") != []:\n",
    "        for textmore in soup.find_all(['div','p'], id=\"news_textmore\"):\n",
    "            article += ('\\n' + textmore.text)\n",
    "    if soup.find_all(['div','p'], class_=\"news_add\") != []:\n",
    "        for newsadd in soup.find_all(['div','p'], class_=\"news_add\"):\n",
    "            if newsadd.h3 != None:\n",
    "                newsadd.h3.extract()\n",
    "            article += ('\\n' + newsadd.text)\n",
    "            \n",
    "    return {\n",
    "        'id':url.split('/')[-1].split('.html')[0],\n",
    "        'title':title.strip(),\n",
    "        'article':article.strip(),\n",
    "        'genre':genre,\n",
    "        'keywords':[],\n",
    "        'url':url,\n",
    "        'datePublished':date,\n",
    "        'dateModified':\"\"\n",
    "    }\n",
    "\n",
    "def get_archiveurl(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    html = str(driver.page_source.encode('utf-8'))\n",
    "    snap = re.search(r'(times between|1 time|times).*?<a href=\"(.+?)\">', html)\n",
    "    #if snap == None:\n",
    "        #return None\n",
    "    archiveurl = 'https://web.archive.org' + snap.group(2)\n",
    "    return archiveurl\n",
    "\n",
    "def js(dic, year):\n",
    "    with open(f'nhkweb{year}.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    with open(f'nhkweb{year}.json', 'w', encoding='utf-8') as f:\n",
    "        if dic['id'] not in [x['id'] for x in data]:\n",
    "            data.append(dic)\n",
    "        else:\n",
    "            for i, d in enumerate(data):\n",
    "                if dic['id'] == d['id']:\n",
    "                    data[i] = dic\n",
    "        data = sorted(data, key=lambda x:x['id'])\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def geturl(year=2019):\n",
    "    idnormal = pd.read_json(f'nhkweb{year}.json', encoding='utf-8')['id'].tolist()\n",
    "    existurl = pd.read_csv('linknormal.txt', encoding='utf-8', header=None)[0].tolist()\n",
    "    nolink = pd.read_csv('nolinknormal.txt', encoding='utf-8', header=None)[0].tolist()\n",
    "    urls = set(existurl) - set(nolink)\n",
    "    return sorted([url for url in urls if (url.split('.html')[0].split('/')[-1] not in idnormal) and f'html/{year}' in url])\n",
    "\n",
    "def checkwrongid(): # check wrong ID in newswebeasy\n",
    "    df = pd.read_json('nhkwebeasy.json', encoding='utf-8')\n",
    "    print(len(df))\n",
    "    df['normalID'] = df['url_normal'].apply(lambda x:x.split('/')[-1].strip('.html'))\n",
    "    return df[df['id'] != df['normalID']]['id'].tolist()\n",
    "\n",
    "def wrongscrape():\n",
    "    wrongids = wrongid()\n",
    "    existurl = pd.read_csv('linknormal.txt', encoding='utf-8', header=None)[0].tolist()[::-1]\n",
    "    for ID in wrongids:\n",
    "        for url in existurl:\n",
    "            if ID in url:\n",
    "                print(url.split('/*/')[-1])\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>article</th>\n      <th>genre</th>\n      <th>keywords</th>\n      <th>url</th>\n      <th>datePublished</th>\n      <th>dateModified</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>k10010000111000</td>\n      <td>過激派組織ＩＳ より過激なグループが主導権か</td>\n      <td>過激派組織ＩＳ＝イスラミックステートがフリージャーナリストの後藤健二さんを殺害したとする映像...</td>\n      <td>[国際]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20150301/k1001...</td>\n      <td>2015-03-01T06:24</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>k10010000141000</td>\n      <td>大学生の生活費 「仕送り」割合過去最低に</td>\n      <td>親元を離れて暮らしている大学生の生活費のうち、「仕送り」が占める割合は５７％と、これまでで最...</td>\n      <td>[社会]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20150301/k1001...</td>\n      <td>2015-03-01T04:23</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>k10010000161000</td>\n      <td>ＰＯＳシステム狙うウイルス 日本で初確認</td>\n      <td>スーパーマーケットなどのレジスターと連動して、商品の売れ行きの把握などに用いられるＰＯＳと呼...</td>\n      <td>[社会]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20150301/k1001...</td>\n      <td>2015-03-01T05:36</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>k10010000181000</td>\n      <td>自民 ゆうちょ銀行の貯金限度額引き上げも視野</td>\n      <td>自民党は、郵政事業に関する新たな特命委員会を設け、１０００万円となっている「ゆうちょ銀行」の...</td>\n      <td>[政治]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20150301/k1001...</td>\n      <td>2015-03-01T05:36</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>k10010000191000</td>\n      <td>“新公文書館は国会周辺”提言骨子案まとまる</td>\n      <td>国の公文書を所蔵する国立公文書館の機能や施設の在り方を検討している政府の有識者会議は、より多...</td>\n      <td>[政治]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20150301/k1001...</td>\n      <td>2015-03-01T04:35</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2848</th>\n      <td>k10015826111000</td>\n      <td>英ウィリアム王子 首相と福島視察</td>\n      <td>安倍総理大臣は日本を訪れているイギリス王室のウィリアム王子と共に福島県を訪れ、原発事故の影響...</td>\n      <td>[社会, ウィリアム, イギリス王室, 王子]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20150228/k1001...</td>\n      <td>2015-02-28T19:15</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2849</th>\n      <td>k10015826211000</td>\n      <td>着床前スクリーニング 研究計画を公表</td>\n      <td>体外受精をしても妊娠できなかったり流産を繰り返したりする女性を対象に、受精卵のすべての染色体...</td>\n      <td>[科学・医療]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20150228/k1001...</td>\n      <td>2015-02-28T19:26</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2850</th>\n      <td>k10015827381000</td>\n      <td>転売巡る指摘 北朝鮮メディアが非難</td>\n      <td>北朝鮮の国営メディアは、朝鮮総連＝在日本朝鮮人総連合会の中央本部の土地と建物の転売を巡り、日...</td>\n      <td>[国際]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20150228/k1001...</td>\n      <td>2015-02-28T22:30</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2851</th>\n      <td>k10015827681000</td>\n      <td>捜索に抵抗し逃走の男 警察に出頭し逮捕</td>\n      <td>今月６日、山口県下関市で麻薬特例法違反の疑いで警察の捜索を受けた男が刃物で抵抗して逃走した事...</td>\n      <td>[社会]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20150228/k1001...</td>\n      <td>2015-02-28T23:16</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2852</th>\n      <td>k10015827781000</td>\n      <td>インド 予算案提出 成長実現の方針強調</td>\n      <td>インド政府は来年度の予算案を議会に提出し、インフラ整備への支出を大幅に増やすとともに、法人税...</td>\n      <td>[経済]</td>\n      <td>[]</td>\n      <td>http://www3.nhk.or.jp/news/html/20150228/k1001...</td>\n      <td>2015-02-28T23:44</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n<p>2853 rows × 8 columns</p>\n</div>",
      "text/plain": "                   id                   title  \\\n0     k10010000111000  過激派組織ＩＳ より過激なグループが主導権か   \n1     k10010000141000    大学生の生活費 「仕送り」割合過去最低に   \n2     k10010000161000    ＰＯＳシステム狙うウイルス 日本で初確認   \n3     k10010000181000  自民 ゆうちょ銀行の貯金限度額引き上げも視野   \n4     k10010000191000   “新公文書館は国会周辺”提言骨子案まとまる   \n...               ...                     ...   \n2848  k10015826111000        英ウィリアム王子 首相と福島視察   \n2849  k10015826211000      着床前スクリーニング 研究計画を公表   \n2850  k10015827381000       転売巡る指摘 北朝鮮メディアが非難   \n2851  k10015827681000     捜索に抵抗し逃走の男 警察に出頭し逮捕   \n2852  k10015827781000     インド 予算案提出 成長実現の方針強調   \n\n                                                article  \\\n0     過激派組織ＩＳ＝イスラミックステートがフリージャーナリストの後藤健二さんを殺害したとする映像...   \n1     親元を離れて暮らしている大学生の生活費のうち、「仕送り」が占める割合は５７％と、これまでで最...   \n2     スーパーマーケットなどのレジスターと連動して、商品の売れ行きの把握などに用いられるＰＯＳと呼...   \n3     自民党は、郵政事業に関する新たな特命委員会を設け、１０００万円となっている「ゆうちょ銀行」の...   \n4     国の公文書を所蔵する国立公文書館の機能や施設の在り方を検討している政府の有識者会議は、より多...   \n...                                                 ...   \n2848  安倍総理大臣は日本を訪れているイギリス王室のウィリアム王子と共に福島県を訪れ、原発事故の影響...   \n2849  体外受精をしても妊娠できなかったり流産を繰り返したりする女性を対象に、受精卵のすべての染色体...   \n2850  北朝鮮の国営メディアは、朝鮮総連＝在日本朝鮮人総連合会の中央本部の土地と建物の転売を巡り、日...   \n2851  今月６日、山口県下関市で麻薬特例法違反の疑いで警察の捜索を受けた男が刃物で抵抗して逃走した事...   \n2852  インド政府は来年度の予算案を議会に提出し、インフラ整備への支出を大幅に増やすとともに、法人税...   \n\n                        genre keywords  \\\n0                        [国際]       []   \n1                        [社会]       []   \n2                        [社会]       []   \n3                        [政治]       []   \n4                        [政治]       []   \n...                       ...      ...   \n2848  [社会, ウィリアム, イギリス王室, 王子]       []   \n2849                  [科学・医療]       []   \n2850                     [国際]       []   \n2851                     [社会]       []   \n2852                     [経済]       []   \n\n                                                    url     datePublished  \\\n0     http://www3.nhk.or.jp/news/html/20150301/k1001...  2015-03-01T06:24   \n1     http://www3.nhk.or.jp/news/html/20150301/k1001...  2015-03-01T04:23   \n2     http://www3.nhk.or.jp/news/html/20150301/k1001...  2015-03-01T05:36   \n3     http://www3.nhk.or.jp/news/html/20150301/k1001...  2015-03-01T05:36   \n4     http://www3.nhk.or.jp/news/html/20150301/k1001...  2015-03-01T04:35   \n...                                                 ...               ...   \n2848  http://www3.nhk.or.jp/news/html/20150228/k1001...  2015-02-28T19:15   \n2849  http://www3.nhk.or.jp/news/html/20150228/k1001...  2015-02-28T19:26   \n2850  http://www3.nhk.or.jp/news/html/20150228/k1001...  2015-02-28T22:30   \n2851  http://www3.nhk.or.jp/news/html/20150228/k1001...  2015-02-28T23:16   \n2852  http://www3.nhk.or.jp/news/html/20150228/k1001...  2015-02-28T23:44   \n\n     dateModified  \n0                  \n1                  \n2                  \n3                  \n4                  \n...           ...  \n2848               \n2849               \n2850               \n2851               \n2852               \n\n[2853 rows x 8 columns]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('nhkweb2015.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "#options.headless = True\n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2d7e969f2c4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_one_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e43317fa45b8>\u001b[0m in \u001b[0;36mscrape_one_new\u001b[0;34m(html, url)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# title, date, genre, keyword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'headline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'contentTitle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datePublished'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'datetime:.*?(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2})'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2d7e969f2c4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_one_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_one_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mjs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mid_exist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e43317fa45b8>\u001b[0m in \u001b[0;36mscrape_one_old\u001b[0;34m(html, url)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# title, date, genre, keyword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"contentTitle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mhmd_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtime_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"news_time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "year = 2015\n",
    "urls = pd.read_csv(f'linknormal.txt', header=None)\n",
    "urls = sorted(urls[urls[0].str.contains(f'html/{year}')][0].tolist())\n",
    "id_exist = set(pd.read_json(f'nhkweb{year}.json')['id'].tolist())\n",
    "\n",
    "for url in urls:\n",
    "    # check URL\n",
    "    ID = url.split('.html')[0].split('/')[-1]\n",
    "    if ID in id_exist:\n",
    "        continue\n",
    "\n",
    "    # get archive URL\n",
    "    archiveurl = get_archiveurl(url)\n",
    "\n",
    "    # request\n",
    "    response = requests.get(archiveurl)\n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "    elif response.status_code == 504:\n",
    "        response = requests.get(archiveurl)\n",
    "        if response.status_code == 504:\n",
    "            continue\n",
    "        html = response.text\n",
    "    time.sleep(4)\n",
    "\n",
    "    # scrape\n",
    "    url_true = 'htt' + url.split('/htt')[-1]\n",
    "\n",
    "    try:\n",
    "        dic = scrape_one_new(html, url_true)\n",
    "    except:\n",
    "        dic = scrape_one_old(html, url_true)\n",
    "    js(dic, year)\n",
    "    id_exist.add(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': 'k10010627901000',\n 'title': '台風５号 北日本～東北の太平洋側 暴風・高波に警戒',\n 'article': '台風５号は日本の東の海上を北上していて、気象庁は北日本から東北にかけての太平洋側を中心に暴風や高波に警戒するよう呼びかけています。\\n気象庁の観測によりますと、台風５号は９日午前６時には、仙台市の東２７０キロの海上を１時間に２５キロの速さで北へ進んでいます。中心の気圧は９８０ヘクトパスカル、最大風速は３０メートル、最大瞬間風速は４０メートルで中心の東側２８０キロ以内と西側１７０キロ以内では風速２５メートル以上の暴風が吹いています。台風はこのあとも北上を続け、９日夜には北海道の東の海上に進み、１０日の朝には千島の近海で温帯低気圧に変わる見込みです。東北の太平洋側では、海はうねりを伴って大しけとなっていて、１０日は北海道の太平洋側でも大しけとなる見込みです。また、北海道の太平洋側では９日夜にかけて非常に強い風が吹き、最大風速は２０メートル、最大瞬間風速は３０メートルに達すると予想されています。気象庁は北日本から東北にかけての太平洋側を中心に、暴風や高波に警戒するよう呼びかけています。',\n 'genre': ['気象', '災害'],\n 'keywords': [],\n 'url': 'http://www3.nhk.or.jp/news/html/20160809/k10010627901000.html',\n 'datePublished': '2016-08-09T06:29',\n 'dateModified': ''}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_true = 'htt' + driver.current_url.split('/htt')[-1]\n",
    "html = driver.page_source.encode('utf-8')\n",
    "try:\n",
    "    dic = scrape_one_new(html, url_true)\n",
    "except:\n",
    "    dic = scrape_one_old(html, url_true)\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "js(dic, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "articles:  10740\n"
    },
    {
     "data": {
      "text/plain": "[('国際', 4223),\n ('社会', 2827),\n ('政治', 1525),\n ('ビジネス', 1341),\n ('スポーツ', 943),\n ('気象・災害', 792),\n ('科学・文化', 783),\n ('暮らし', 519),\n ('地域', 423)]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check category\n",
    "\n",
    "year = 2017\n",
    "\n",
    "with open(f'nhkweb{year}.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print('articles: ', len(data))\n",
    "genre = Counter()\n",
    "for dic in data:\n",
    "    for g in dic['genre']:\n",
    "        genre[g] += 1\n",
    "genre.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genre <> keywords\n",
    "with open(f'nhkweb{year}.json','r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "category = ['社会', '国際', 'ビジネス', 'スポーツ', '政治', '科学・文化', '暮らし', '地域', '気象・災害']\n",
    "for i, dic in enumerate(data):\n",
    "    newgenre = []\n",
    "    newkey = []\n",
    "    for j in dic['genre']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        elif j == \"科学・医療\" or j == \"文化・エンタメ\" or j == \"科学\":\n",
    "            newgenre.append('科学・文化')\n",
    "        elif j == \"暮らし文化\":\n",
    "            newgenre.append('暮らし')\n",
    "            newgenre.append('科学・文化')\n",
    "        elif j == \"経済\":\n",
    "            newgenre.append('ビジネス')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    for j in dic['keywords']:\n",
    "        if j in category:\n",
    "            newgenre.append(j)\n",
    "        elif j == \"科学・医療\" or j == \"文化・エンタメ\" or j == \"科学\":\n",
    "            newgenre.append('科学・文化')\n",
    "        elif j == \"暮らし文化\":\n",
    "            newgenre.append('暮らし')\n",
    "            newgenre.append('科学・文化')\n",
    "        elif j == \"災害\" or j == \"気象\":\n",
    "            newgenre.append('気象・災害')\n",
    "        else:\n",
    "            newkey.append(j)\n",
    "    data[i]['genre'] = list(set(newgenre))\n",
    "    data[i]['keywords'] = list(set(newkey))\n",
    "\n",
    "with open(f'nhkweb{year}.json','w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "k10011141491000    1\nk10011000931000    1\nk10010934981000    1\nk10010951261000    1\nk10010933321000    1\n                  ..\nk10011177701000    1\nk10011195271000    1\nk10010962891000    1\nk10011073211000    1\nk10011211931000    1\nName: id, Length: 10740, dtype: int64"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(f'nhkweb{year}.json').id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}